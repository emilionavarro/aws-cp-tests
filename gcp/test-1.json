[
    {
      "options": [
        "Use External TCP proxy load balancer.",
        "Use External SSL proxy load balancer.",
        "Use HTTP(S) load balancer.",
        "Use Internal TCP load balancer."
      ],
      "correct": "Use HTTP(S) load balancer.",
      "questionText": "You want to migrate a public NodeJS application, which serves requests over HTTPS, from your on-premises data centre to Google Cloud Platform. You plan to host it on a fleet of instances behind Managed Instances Group (MIG) in Google Compute Engine. You need to configure a GCP load balancer to terminate SSL session before passing traffic to the VMs. Which GCP Load balancer should you use?",
      "explanation": "Use Internal TCP load balancer. is not right.\nInternal TCP Load Balancing is a regional load balancer that enables you to run and scale your services behind an internal load balancing IP address that is accessible only to your internal virtual machine (VM) instances. Since we need to serve public traffic, this load balancer is not suitable for us.\nRef: https://cloud.google.com/load-balancing/docs/internal\n\nUse External SSL proxy load balancer. is not right.\nGoogle says \"SSL Proxy Load Balancing is intended for non-HTTP(S) traffic. For HTTP(S) traffic, we recommend that you use HTTP(S) Load Balancing.\" So this option can be ruled out.\nRef: https://cloud.google.com/load-balancing/docs/ssl\n\nUse External TCP proxy load balancer. is not right.\nGoogle says \"TCP Proxy Load Balancing is intended for non-HTTP traffic. For HTTP traffic, use HTTP Load Balancing instead. For proxied SSL traffic, use SSL Proxy Load Balancing.\" So this option can be ruled out.\nRef: https://cloud.google.com/load-balancing/docs/tcp\n\nUse HTTP(S) load balancer. is the right answer.\nThis option fits all requirements. It can serve public traffic, can terminate SSL at the load balancer and follows google recommended practices.\n\n\"The backends of a backend service can be either instance groups or network endpoint groups (NEGs), but not a combination of both.\"\n\n\"An external HTTP(S) load balancer distributes traffic from the internet.\"\n\n\"The client SSL session terminates at the load balancer.\"\n\n\"For HTTP traffic, use HTTP Load Balancing instead.\"\n\nRef: https://cloud.google.com/load-balancing/docs/https"
    },
    {
      "options": [
        "Copy the zip file to a new Cloud Storage bucket, make the bucket public and share the URL securely with the external auditor. Delete the new bucket after 4 hours.",
        "Make the zip file public and securely share the URL with the external auditor. Set up a lifecycle policy to delete the object after 4 hours.",
        "Configure Static Website hosting on the Cloud Storage bucket, make the zip file public and ask the auditor to download the file from the website. Delete the zip file after 4 hours.",
        "Use gcloud to generate a signed URL on the object with a four-hour expiry. Securely share the URL with the external auditor."
      ],
      "correct": "Use gcloud to generate a signed URL on the object with a four-hour expiry. Securely share the URL with the external auditor.",
      "questionText": "Your compliance department has asked you to share a compressed zip of sensitive audit logs with an external auditor. The external auditor does not have a Google account, and you want to remove the access after 4 hours. How can you do this securely with the least number of steps?",
      "explanation": "Make the zip file public and securely share the URL with the external auditor. Set up a lifecycle policy to delete the object after 4 hours. is not right.\nWhile the external company can access the public objects from the bucket, it doesn't stop bad actors from accessing the data as well. Since the data is \"sensitive\" and we want to follow a \"secure method\", we shouldn't do this.\n\nConfigure Static Website hosting on the Cloud Storage bucket, make the zip file public and ask the auditor to download the file from the website. Delete the zip file after 4 hours. is not right.\nThe static website is public by default. While the external company can access the objects from the static website, it doesn't stop bad actors from accessing the data as well. Since the data is \"sensitive\" and we want to follow a \"secure method\", we shouldn't do this.\n\nCopy the zip file to a new Cloud Storage bucket, make the bucket public and share the URL securely with the external auditor. Delete the new bucket after 4 hours. is not right.\nEven if we were to create a separate bucket for the external company, the external auditor can’t access as they do not have a google account. The only way to have them access this separate bucket is by enabling public access which we can't because of the nature of data (sensitive) and is against standard security practices.\n\nUse gcloud to generate a signed URL on the object with a four-hour expiry. Securely share the URL with the external auditor. is the right answer.\nThis option fits all requirements. When we generate a signed URL, we can specify an expiry and only users with the signed URL can view/download the objects, and they don't need a google account.\nRef: https://cloud.google.com/storage/docs/access-control/signed-urls\nThis page provides an overview of signed URLs, which you use to give time-limited resource access to anyone in possession of the URL, regardless of whether they have a Google account."
    },
    {
      "options": [
        "Use Deployment Manager to create two VPCs, each with a subnet in a different region. Ensure the subnets use non-overlapping IP range.",
        "Use Deployment Manager to create a new VPC with 2 subnets in the same region. Ensure the subnets use the same IP range.",
        "Use Deployment Manager to create a new VPC with 2 subnets in 2 different regions. Ensure the subnets use non-overlapping IP range.",
        "Use Deployment Manager to create two VPCs, each with a subnet in the same region. Ensure the subnets use overlapping IP range."
      ],
      "correct": "Use Deployment Manager to create a new VPC with 2 subnets in 2 different regions. Ensure the subnets use non-overlapping IP range.",
      "questionText": "You are migrating your on-premises workloads to GCP VPC, and you want to use Compute Engine virtual machines. You want to separate the Finance team VMs and the Procurement team VMs into separate subnets. You need all VMs to communicate with each other over their internal IP addresses without adding routes. What should you do?",
      "explanation": "Use Deployment Manager to create two VPCs, each with a subnet in a different region. Ensure the subnets use non-overlapping IP range. is not right.\nWe need to get our requirements working with 1 VPC, not 2 !!\n\nUse Deployment Manager to create two VPCs, each with a subnet in the same region. Ensure the subnets use overlapping IP range. is not right.\nWe need to get our requirements working with 1 VPC, not 2 !!\n\nUse Deployment Manager to create a new VPC with 2 subnets in the same region. Ensure the subnets use the same IP range. is not right.\nWe can not create two subnets in one VPC with the same CIDR range. \"Primary and secondary ranges for subnets cannot overlap with any allocated range, any primary or secondary range of another subnet in the same network, or any IP ranges of subnets in peered networks.\"\nRef: https://cloud.google.com/vpc/docs/using-vpc#subnet-rules\n\nUse Deployment Manager to create a new VPC with 2 subnets in 2 different regions. Ensure the subnets use non-overlapping IP range. is the right answer.\nWhen we create subnets in the same VPC with different CIDR ranges, they can communicate automatically within VPC. \"Resources within a VPC network can communicate with one another by using internal (private) IPv4 addresses, subject to applicable network firewall rules.\"\nRef: https://cloud.google.com/vpc/docs/vpc"
    },
    {
      "options": [
        "Add a metadata tag to the instance with key: new-memory-size and value: 8GB.",
        "Stop the compute engine instance, update the memory on the instance to 8 GB and start the compute engine instance.",
        "Stop the compute engine instance, update the machine to n1-standard-2 and start the compute engine instance.",
        "Make use of the live-migration feature of Google Compute Engine to migrate the application to another instance with more memory."
      ],
      "correct": "Stop the compute engine instance, update the memory on the instance to 8 GB and start the compute engine instance.",
      "questionText": "You deployed a Java application in a Google Compute Engine VM that has 3.75 GB Memory and 1 vCPU. At peak usage, the application experiences java.lang.OutOfMemory errors that take down the application entirely and requires a restart. The CPU usage at all times is minimal. Your operations team have asked you to increase the memory on the VM instance to 8 GB. You want to do this while minimizing the cost. What should you do?",
      "explanation": "Make use of the live-migration feature of Google Compute Engine to migrate the application to another instance with more memory. is not right.\nLive migration migrates your running instances to another host in the same zone so that Google can perform maintenance such as a software or hardware update. It can not be used for changing machine type.\nRef: https://cloud.google.com/compute/docs/instances/live-migration\n\nAdd a metadata tag to the instance with key: new-memory-size and value: 8GB. is not right.\nThere is no such setting as new-memory-size.\n\nStop the compute engine instance, update the machine to n1-standard-2 and start the compute engine instance. is not right.\nn1-standard-2 instance offers less than 8 GB (7.5 GB to be precise), so this falls short of the required memory.\nRef: https://cloud.google.com/compute/docs/machine-types\n\nStop the compute engine instance, update the memory on the instance to 8 GB and start the compute engine instance. is the right answer.\nIn Google compute engine, if predefined machine types don't meet your needs, you can create an instance with custom virtualized hardware settings. Specifically, you can create an instance with a custom number of vCPUs and custom memory, effectively using a custom machine type. Custom machine types are ideal for the following scenarios:\n\nWorkloads that aren't a good fit for the predefined machine types that are available to you.\n\nWorkloads that require more processing power or more memory but don't need all of the upgrades that are provided by the next machine type level.\n\nIn our scenario, we only need a memory upgrade. Moving to a bigger instance would also bump up the CPU which we don't need so we have to use a custom machine type. It is not possible to change memory while the instance is running, so you need to stop the instance first, change the memory and then start it again.\nRef: https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type"
    },
    {
      "options": [
        "Add a firewall rule to allow TCP traffic on port 22. In the GCP console, add a password for the Windows VM instance. Install Chrome RDP for Google Cloud Platform extension and click the RDP button in the console to connect to the instance with the credentials.",
        "In the GCP console, add a username and password for the Windows VM instance. Install an RDP client and connect to the instance with username and password.",
        "Add a firewall rule to allow TCP traffic on port 3389. Install an RDP client and connect to the instance.",
        "Add a firewall rule to allow TCP traffic on port 3389. In the GCP console, add a username and password for the Windows VM instance. Install Chrome RDP for Google Cloud Platform extension and click the RDP button in the console to connect to the instance with the credentials."
      ],
      "correct": "Add a firewall rule to allow TCP traffic on port 3389. In the GCP console, add a username and password for the Windows VM instance. Install Chrome RDP for Google Cloud Platform extension and click the RDP button in the console to connect to the instance with the credentials.",
      "questionText": "You are enhancing a production application currently running on an Ubuntu Linux VM on Google Compute Engine. The new enhancements require a connection to SQL Server instance to persist user appointments. Your colleague has provisioned an SQL Server instance in a Google Compute Engine VM in US-Central region and has asked for your assistance to RDP to the VM in the least number of steps. What should you suggest?",
      "explanation": "Requirements - Connect to compute instance using fewest steps. The presence of SQL Server 2017 on the instance is a red herring and should be ignored as none of the options provided say anything about the database and all seem to revolve around RDP.\n\nAdd a firewall rule to allow TCP traffic on port 3389. Install an RDP client and connect to the instance. is not right.\nAlthough opening port 3389 is essential for serving RDP traffic, we do not have the credentials to RDP, so this isn’t going to work.\n\nAdd a firewall rule to allow TCP traffic on port 22. In the GCP console, add a password for the Windows VM instance. Install Chrome RDP for Google Cloud Platform extension and click the RDP button in the console to connect to the instance with the credentials. is not right.\nRDP uses port 3389 and not 22.\nRef: https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-rdp\n\nIn the GCP console, add a username and password for the Windows VM instance. Install an RDP client and connect to the instance with username and password. is not right.\nThis option correctly sets the username/password, which is essential. However, the port on 3389 has not been opened to allow RDP. Therefore, RDP connection from an external client fails.\n\nAdd a firewall rule to allow TCP traffic on port 3389. In the GCP console, add a username and password for the Windows VM instance. Install Chrome RDP for Google Cloud Platform extension and click the RDP button in the console to connect to the instance with the credentials. is the right answer.\nThis option correctly sets the username and password on the console and verifies a firewall rule is set on port 3389 to allow RDP traffic. You also install Chrome RDP for Google Cloud Platform extension to RDP from the console. (See Chrome Desktop for GCP tab in https://cloud.google.com/compute/docs/instances/connecting-to-instance#windows) which lets you click on the RDP button to launch an RDP session where you will be prompted for a username and password."
    },
    {
      "options": [
        "Enable autoscaling on the Managed Instance Group (MIG) and set minimum instances to 1 and maximum instances to 1.",
        "Disable autoscaling on the Managed Instance Group (MIG) and set mininum instances to 1 and maximum instances to 2.",
        "Disable autoscaling on the Managed Instance Group (MIG) and set mininum instances to 1 and maximum instances to 1.",
        "Enable autoscaling on the Managed Instance Group (MIG) and set minimum instances to 1 and maximum instances to 2."
      ],
      "correct": "Enable autoscaling on the Managed Instance Group (MIG) and set minimum instances to 1 and maximum instances to 1.",
      "questionText": "You want to deploy a cost-sensitive application to Google Cloud Compute Engine. You want the application to be up at all times, but because of the cost-sensitive nature of the application, you only want to run the application in a single VM instance. How should you configure the managed instance group?",
      "explanation": "Requirements\n\n1. Since we need the application running at all times, we need a minimum 1 instance.\n\n2. Only a single instance of the VM should run, we need a maximum 1 instance.\n\nWe want the application running at all times. If the VM crashes due to any underlying hardware failure, we want another instance to be added to MIG so that application can continue to serve requests. We can achieve this by enabling autoscaling.\n\nThe only option that satisfies these three is Enable autoscaling on the Managed Instance Group (MIG) and set minimum instances to 1 and maximum instances to 1.\nRef: https://cloud.google.com/compute/docs/autoscaler"
    },
    {
      "options": [
        "Use a Cloud Function to rewrite the storage class to Coldline for objects older than 90 days. Use another Cloud Function to delete objects older than 275 days from Coldline Storage Class.",
        "Use a Cloud Function to rewrite the storage class to Coldline for objects older than 90 days. Use another Cloud Function to delete objects older than 365 days from Coldline Storage Class.",
        "Configure a lifecycle rule to transition objects older than 90 days to Coldline Storage Class. Configure another lifecycle rule to delete objects older than 365 days from Coldline Storage Class.",
        "Configure a lifecycle rule to transition objects older than 90 days to Coldline Storage Class. Configure another lifecycle rule to delete objects older than 275 days from Coldline Storage Class."
      ],
      "correct": "Configure a lifecycle rule to transition objects older than 90 days to Coldline Storage Class. Configure another lifecycle rule to delete objects older than 365 days from Coldline Storage Class.",
      "questionText": "Your company produces documentary videos for a reputed television channel and stores its videos in Google Cloud Storage for long term archival. Videos older than 90 days are accessed only in exceptional circumstances and videos older than one year are no longer needed. How should you optimise the storage to reduce costs?",
      "explanation": "Use a Cloud Function to rewrite the storage class to Coldline for objects older than 90 days. Use another Cloud Function to delete objects older than 365 days from Coldline Storage Class. is not right.\ngsutil rewrite is used to change the storage class of objects within a bucket through overwriting the object. Upon overwriting, the age on the objects is reset. Deleting them after 365 days would result in the original objects being deleted in 90+365 days.\nRef: https://cloud.google.com/storage/docs/changing-storage-classes\n\nUse a Cloud Function to rewrite the storage class to Coldline for objects older than 90 days. Use another Cloud Function to delete objects older than 275 days from Coldline Storage Class. is not right.\ngsutil rewrite is used to change the storage class of objects within a bucket through overwriting the object. Upon overwriting, the age on the objects is reset. Deleting them after 275 days would result in the original objects being deleted in 90+275 days. Although this is the expected outcome, you should avoid doing this as the Cloud Storage lifecycle management rules provide a better way to achieve this without relying on other GCP services.\nRef: https://cloud.google.com/storage/docs/changing-storage-classes\n\nConfigure a lifecycle rule to transition objects older than 90 days to Coldline Storage Class. Configure another lifecycle rule to delete objects older than 275 days from Coldline Storage Class. is not right.\nObject Lifecycle Management does not rewrite an object when changing its storage class. When an object is transitioned to Nearline Storage, Coldline Storage, or Archive Storage using the SetStorageClass feature, any subsequent early deletion and associated charges are based on the original creation time of the object, regardless of when the storage class changed.\n\nIf however, the change of storage class is done manually using a rewrite, the creation time of the objects is the new creation time since they are rewritten. In such a case, you would need to apply a lifecycle delete action of 275 days.\nRef: https://cloud.google.com/storage/docs/lifecycle\n\nConfigure a lifecycle rule to transition objects older than 90 days to Coldline Storage Class. Configure another lifecycle rule to delete objects older than 365 days from Coldline Storage Class. is the right answer.\nObject Lifecycle Management does not rewrite an object when changing its storage class. When an object is transitioned to Nearline Storage, Coldline Storage, or Archive Storage using the SetStorageClass feature, any subsequent early deletion and associated charges are based on the original creation time of the object, regardless of when the storage class changed.\nRef: https://cloud.google.com/storage/docs/lifecycle"
    },
    {
      "options": [
        "Cloud Pub/Sub for ingesting, Cloud Storage for transforming, BigQuery for storing and Cloud Bigtable for analyzing the time-series data.",
        "Cloud Pub/Sub for ingesting, Cloud Dataflow for transforming, Cloud Bigtable for storing and BigQuery for analyzing the time-series data.",
        "Cloud Pub/Sub for ingesting, Cloud Dataflow for transforming, Cloud Datastore for storing and BigQuery for analyzing the time-series data.",
        "Firebase Messages for ingesting, Cloud Pub/Sub for transforming, Cloud Spanner for storing and BigQuery for analyzing the time-series data."
      ],
      "correct": "Cloud Pub/Sub for ingesting, Cloud Dataflow for transforming, Cloud Bigtable for storing and BigQuery for analyzing the time-series data.",
      "questionText": "Your company installs and manages several types of IoT devices all over the world. Events range from 50,000 to 500,000 messages a second. You want to identify the best solution for ingesting, transforming, storing and analyzing this data in GCP platform. What GCP services should you use?",
      "explanation": "Cloud Pub/Sub for ingesting, Cloud Dataflow for transforming, Cloud Bigtable for storing and BigQuery for analyzing the time-series data. is the right answer.\nFor ingesting time series data, your best bet is Cloud Pub/Sub. For processing the data in pipelines, your best bet is Cloud Dataflow.\n\nThat leaves us with two remaining options; both have BigQuery for analyzing the data. For storage, it is a choice between Bigtable and Datastore. Bigtable provides out of the box support for time series data. So using Bigtable for Storage is the right answer.\nRef: https://cloud.google.com/bigtable/docs/schema-design-time-series"
    },
    {
      "options": [
        "Increase the transfer speed by decreasing the TCP window size.",
        "Restart the transfer from GCP console.",
        "Upload the file Multi-Regional instead and move the file to Nearline Storage Class.",
        "Use parallel composite uploads to speed up the transfer."
      ],
      "correct": "Use parallel composite uploads to speed up the transfer.",
      "questionText": "Your company is migrating an application from its on-premises data centre to Google Cloud. One of the applications uses a custom Linux distribution that is not available on Google Cloud. Your solution architect has suggested using VMWare tools to exporting the image and store it in a Cloud Storage bucket. The VM Image is a single compressed 64 GB tar file. You started copying this file using gsutil over a dedicated 1Gbps network, but the transfer is taking a very long time to complete. Your solution architect has suggested using all of the 1Gbps Network to transfer the file quickly. What should you do?",
      "explanation": "Requirements - transfer the file rapidly, use as much of the rated 1 Gbps as possible\n\nRestart the transfer from GCP console. is not right.\nGCP Console does not offer any specific features that help in improving the upload speed.\n\nIncrease the transfer speed by decreasing the TCP window size. is not right.\nBy decreasing the TCP window size, you are reducing the chunks of data sent in the TCP window, and this has the effect of underutilizing your bandwidth and can slow down the upload.\n\nUpload the file Multi-Regional instead and move the file to Nearline Storage Class. is not right.\nMulti-Regional is not a storage class. It is a bucket location. You can transition between storage classes, but that does not improve the upload speed.\nRef: https://cloud.google.com/storage/docs/locations\nRef: https://cloud.google.com/storage/docs/storage-classes\n\nUse parallel composite uploads to speed up the transfer. is the right answer.\nWith cloud storage, Object composition can be used for uploading an object in parallel: you can divide your data into multiple chunks, upload each chunk to a distinct object in parallel, compose your final object, and delete any temporary source objects. This option helps maximize your bandwidth usage and ensures the file is uploaded as fast as possible.\nRef: https://cloud.google.com/storage/docs/composite-objects#uploads"
    },
    {
      "options": [
        "Update the Managed Instances template to set the maximum instances to 1.",
        "Update the autoscaling health check to increase the initial delay to 200 seconds.",
        "Update the autoscaling health check from HTTP to TCP.",
        "Update the Managed Instances template to set the maximum instances to 5."
      ],
      "correct": "Update the autoscaling health check to increase the initial delay to 200 seconds.",
      "questionText": "Your team manages the game backend for a popular game with users all over the world. The game backend APIs runs on a fleet of VMs behind a Managed Instance Group (MIG) with autoscaling enabled. You have configured the scaling policy on the MIG to add more instances if the CPU utilization is consistently over 85%, and to scale down when the CPU utilization is consistently lower than 65%. You noticed the autoscaler adds more VMs than is necessary during the scale-up, and you suspect this might be down to an incorrect configuration in the health check – the initial delay on the health check is 30 seconds. Each VM takes just under 3 minutes before it is ready to process the requests from the web application and mobile app. What should you do to fix the scaling issue?",
      "explanation": "Scenario\n\n- Autoscaling is enabled and kicks off the scale-up\n\n- Scaling policy is based on target CPU utilization of 80%\n\n- The initial delay is 30 seconds.\n\n- VM startup time is 3 minutes.\n\n- Auto-scaling creates more instances than necessary.\n\nUpdate the Managed Instances template to set the maximum instances to 1. is not right.\nSetting the maximum number of instances to 1 effectively limits the scale up to 1 instance, which is undesirable as in this case, we may still be struggling with the CPU usage and can't scale up. Therefore this is not the right answer.\n\nUpdate the Managed Instances template to set the maximum instances to 5. is not right.\nSetting the maximum number of instances to 5 effectively limits the scale up to 5 instances. In this scenario, we may still be struggling with CPU usage and can't scale up. Therefore this is not the right answer.\n\nUpdate the autoscaling health check from HTTP to TCP. is not right.\nA TCP health check is a legacy health check, whereas HTTP health check is more advanced and \"non-legacy\". A TCP health check might say the application is UP when it is not as it only listens on application servers TCP port and doesn't validate the application health through HTTP check on its health endpoint. This configuration results in the load balancer sending requests to the application server when it is still loading the application resulting in failures.\nRef: https://cloud.google.com/sdk/gcloud/reference/compute/health-checks/create/tcp\nRef: https://cloud.google.com/sdk/gcloud/reference/compute/health-checks/create/http\n\nUpdate the autoscaling health check to increase the initial delay to 200 seconds. is the right answer.\nThe reason why our autoscaling is adding more instances than needed is that it checks 30 seconds after launching the instance, and at this point, the instance isn't up and isn't ready to serve traffic. So our autoscaling policy starts another instance - again checks this after 30 seconds and the cycle repeats until it gets to the maximum instances or the instances launched earlier are healthy and start processing traffic - which happens after 180 seconds (3 minutes). This issue can be easily rectified by adjusting the initial delay to be higher than the time it takes for the instance to become available for processing traffic.\n\nSo setting this to 200 ensures that it waits until the instance is up (around 180-second mark) and then starts forwarding traffic to this instance. Even after the cool out period, if the CPU utilization is still high, the autoscaler can again scale up, but this scale-up is genuine and is based on the actual load.\n\n\"Initial Delay Seconds\" - This setting delays autohealing from potentially prematurely recreating the instance if the instance is in the process of starting up. The initial delay timer starts when the currentAction of the instance is VERIFYING.\nRef: https://cloud.google.com/compute/docs/instance-groups/autohealing-instances-in-migs"
    },
    {
      "options": [
        "Check the output of gcloud iam service-accounts list command.",
        "Review the information in the Roles section for the production GCP project in Google Cloud Console.",
        "Review the information in the IAM section for the production GCP project in Google Cloud Console.",
        "Check the output of gcloud iam roles list command."
      ],
      "correct": "Review the information in the IAM section for the production GCP project in Google Cloud Console.",
      "questionText": "You are the Cloud Security Manager at your company, and you want to review IAM users and their assigned roles in the production GCP project. You want to follow Google recommended practices. What should you do?",
      "explanation": "Requirements - verify users (i.e. IAM members) and roles.\n\nCheck the output of gcloud iam roles list command. is not right.\ngcloud iam roles list lists the roles but does not list the users (i.e. IAM members)\n\nCheck the output of gcloud iam service-accounts list command. is not right.\ngcloud iam service-accounts list lists the service accounts which are users (i.e. IAM members), but it ignores other users that are not service accounts, e.g. users in GSuite domain, or groups etc.\n\nReview the information in the Roles section for the production GCP project in Google Cloud Console. is not right.\nThis option allows us to review the roles but not users. See the screenshot below.\n\nReview the information in the IAM section for the production GCP project in Google Cloud Console. is the right answer.\n\nThis option that lets us view roles as well as users (members).\n\nRef: https://cloud.google.com/iam/docs/overview\n\nSee the screenshot below.\n\nA member can be a Google Account (for end-users), a service account (for apps and virtual machines), a Google group, or a G Suite or Cloud Identity domain that can access a resource. The identity of a member is an email address associated with a user, service account, or Google group; or a domain name associated with G Suite or Cloud Identity domains."
    },
    {
      "options": [
        "Deploy the update in a new MIG and add it as a backend service to the existing production Load Balancer. Once all instances in the new group have warmed up, remove the old MIG from the Load Balancer backend and delete the group.",
        "Update the existing Managed Instance Group (MIG) to point to a new instance template containing the updated version. Terminate all existing instances in the MIG and wait until they are all replaced by new instances created from the new template.",
        "Carry out a rolling update by executing gcloud compute instance-groups {managed group name} rolling-action start-update --max-surge 0 --max-unavailable 1.",
        "Carry out a rolling update by executing gcloud compute instance-groups {managed group name} rolling-action start-update --max-surge 1 --max-unavailable 0.\n"
      ],
      "correct": "Carry out a rolling update by executing gcloud compute instance-groups {managed group name} rolling-action start-update --max-surge 1 --max-unavailable 0.\n",
      "questionText": "A production application serving live traffic needs an important update deployed gradually. The application is deployed in a Managed Instance Group (MIG) in the US-Central region. The application receives millions of requests each minute, and you want to patch the application while ensuring the number of instances (capacity) in the Managed Instance Group (MIG) does not decrease. What should you do?",
      "explanation": "Our requirements are\n\nDeploy a new version gradually and\n\nEnsure available capacity does not decrease during deployment.\n\nUpdate the existing instance template with the required changes and deploy the changes to a new MIG. Update the existing production Load Balancer configuration to add the newly created MIG as a backend service. Once all instances in the new group have warmed up, remove the old MIG from the Load Balancer backend. When all instances belonging to the old MIG have been drained, delete the OLD mig. is not right.\nFirst of all instance, instance templates can not be updated. So the phrase Update the existing instance template rules out this option.\nRef: https://cloud.google.com/compute/docs/instance-templates/\n\nUpdate the existing Managed Instance Group (MIG) to point to a new instance template containing the updated version. Terminate all existing instances in the MIG and wait until they are all replaced by new instances created from the new instance template. is not right.\nIf we follow these steps, we end up with a full fleet of instances belonging to the new managed instances group (i.e. based on the new template) behind the load balancer. Our requirement to gradually deploy the new version is not met. Also, deleting the existing instances of the managed instance group would almost certainly result in an outage to our application which is not desirable when we are serving live web traffic.\n\nCarry out a rolling update by executing gcloud compute instance-groups {managed group name} rolling-action start-update --max-surge 0 --max-unavailable 1. is not right.\nmaxSurge specifies the maximum number of instances that can be created over the desired number of instances. If maxSurge is set to 0, the rolling update can not create additional instances and is forced to update existing instances resulting in a reduction in capacity. Therefore, it does not satisfy our requirement to ensure that the available capacity does not decrease during the deployment. maxUnavailable - specifies the maximum number of instances that can be unavailable during the update process. When maxUnavailable is set to 1, the rolling update updates 1 instance at a time. i.e. it takes 1 instance out of service, updates it, and puts it back into service. This option results in a reduction in capacity while the instance is out of service. Example - if we have 10 instances in service, this combination of setting results in 1 instance at a time taken out of service for an upgrade while the remaining 9 continue to serve live traffic. That’s a reduction of 10% in available capacity and does not satisfy our requirement to ensure that the available capacity does not decrease during the deployment.\n\nCarry out a rolling update by executing gcloud compute instance-groups {managed group name} rolling-action start-update --max-surge 1 --max-unavailable 0. is the right answer.\nThis option is the only one that satisfies our two requirements - deploying gradually and ensuring the available capacity does not decrease. When maxUnavailable is set to 0, the rolling update can not take existing instances out of service. And when maxSurge is set to 1, we let the rolling update spin a single additional instance. The rolling update then puts the additional instance into service and takes one of the existing instances out of service for the upgrade. There is no reduction in capacity at any point in time. And the rolling upgrade upgrades 1 instance at a time, so we gradually deploy the new version. As an example - if we have 10 instances in service, this combination of setting results in 1 additional instance put into service (resulting in 11 instances serving traffic), then an older instance taken out of service (resulting in 10 instances serving traffic) and the upgraded instance put back into service (resulting in 11 instances serving traffic). The rolling upgrade continues updating the remaining 9 instances one at a time. Finally, when all 10 instances have been upgraded, the additional instance that is spun up is deleted. We still have 10 instances serving live traffic but now on the new version of code.\nRef: https://cloud.google.com/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups"
    },
    {
      "options": [
        "Link the GCP project to a Cloud Monitoring workspace. Configure an Alerting policy based on CPU utilization in Cloud Monitoring and trigger an email notification when the utilization exceeds the threshold.",
        "Link the project to a Cloud Monitoring workspace. Write a custom script that captures CPU utilization every minute and sends to Cloud Monitoring as a custom metric. Add an uptime check based on the CPU utilization.",
        "Write a custom script to monitor CPU usage and send an email notification when the usage exceeds the threshold.",
        "In Cloud Logging, create logs based metric for CPU usage and store it as a custom metric in Cloud Monitoring. Create an Alerting policy based on CPU utilization in Cloud Monitoring and trigger an email notification when the utilization exceeds the threshold."
      ],
      "correct": "Link the GCP project to a Cloud Monitoring workspace. Configure an Alerting policy based on CPU utilization in Cloud Monitoring and trigger an email notification when the utilization exceeds the threshold.",
      "questionText": "You deployed a java application in a single Google Cloud Compute Engine VM. During peak usage, the application CPU is maxed out and results in stuck threads which ultimately make the system unresponsive, and requires a reboot. Your operations team want to receive an email alert when the CPU utilization is greater than 95% for more than 10 minutes so they can manually change the instance type to another instance that offers more CPU. What should you do?",
      "explanation": "We want to use Google services. So that eliminates the two options where we Write a script. Why would we want to write a script when there is a Google service that does precisely that - with minimal configuration!!\n\nCloud logging does not log CPU usage. (Cloud monitoring does that) So that rules out the other option.\nRef: https://cloud.google.com/logging/\n\nLink the GCP project to a Cloud Monitoring workspace. Configure an Alerting policy based on CPU utilization in Cloud Monitoring and trigger an email notification when the utilization exceeds the threshold. is the right answer.\n\nA Workspace is a tool for monitoring resources contained in one or more Google Cloud projects or AWS accounts. In our case, we create a Stackdriver workspace and link our project to this workspace.\nRef: https://cloud.google.com/monitoring/workspaces\n\nCloud monitoring captures the CPU usage. By default, the Monitoring agent collects disk, CPU, network, and process metrics. You can also have the agent send custom metrics to Cloud monitoring.\nRef: https://cloud.google.com/monitoring/\n\nYou can then set up an alerting policy to alert with CPU utilization exceeds 90% for 15 minutes.\nRef: https://cloud.google.com/monitoring/alerts/.\n\nSee here for an example of setting up an alerting policy on CPU load. In our case, we’d have to substitute the CPU load for the CPU utilization metric.\nRef: https://cloud.google.com/monitoring/quickstart-lamp\n\nCloud monitoring supports multiple notification options for triggering alerts; email is one of them.\nRef: https://cloud.google.com/monitoring/support/notification-options"
    },
    {
      "options": [
        "Create an instance template with availability policy that turns on the automatic restart behaviour and sets on-host maintenance to live migrate instances during maintenance events. Deploy the application on a Managed Instance Group (MIG) based on this template.",
        "Deploy the application on a Managed Instance Group (MIG) with autohealing health check set to healthy (HTTP).",
        "Create an instance template with availability policy that turns off the automatic restart behaviour and sets on-host maintenance to terminate instances during maintenance events. Deploy the application on a Managed Instance Group (MIG) based on this template.",
        "Deploy the application on a Managed Instance Group (MIG) that disables the creation retry mode by setting the --nocreation-retries flag."
      ],
      "correct": "Create an instance template with availability policy that turns on the automatic restart behaviour and sets on-host maintenance to live migrate instances during maintenance events. Deploy the application on a Managed Instance Group (MIG) based on this template.",
      "questionText": "Your company is migrating a mission-critical application from the on-premises data centre to Google Cloud Platform. The application requires 12 Compute Engine VMs to handle traffic at peak usage times. Your operations team have asked you to ensure the VMs restart automatically (i.e. without manual intervention) if/when they crash, and the processing capacity of the application does not reduce down during system maintenance. What should you do?",
      "explanation": "Requirements\n\n12 instances - indicates we need to look for MIG (Managed Instances Group) where we can configure healing/scaling settings.\n\nHighly available during system maintenance - indicates we need to look for Live Migration.\n\nAutomatically restart on crash - indicates we need to look for options that enable automatic restarts.\n\nCreate an instance template with availability policy that turns off the automatic restart behaviour and sets on-host maintenance to terminate instances during maintenance events. Deploy the application on a Managed Instance Group (MIG) based on this template. is not right.\nIf Automatic Restart is off, then the compute engine instances are not automatically restarted and results in loss of capacity. If GCP decides to start system maintenance on all instances at the same time, all instances are down, and this does not meet our requirement \"Highly available during system maintenance\" so this option is not right.\n\nDeploy the application on a Managed Instance Group (MIG) with autohealing health check set to healthy (HTTP). is not right.\nWhile auto-healing helps with the recreation of VM instances when needed, it doesn't Live-migrate the instances so our requirement of \"highly available including during system maintenance\" is not met. More info about Autohealing - Auto-healing allows the recreation of VM instances when needed. You can use a health check to recreate a VM instance if the health check finds it unresponsive. If you don't select a health check, Compute Engine will recreate VM instances only when they're not running.\nRef: https://cloud.google.com/compute/docs/instance-groups/?hl=en_GB#managed_instance_groups_and_autohealing\n\nDeploy the application on a Managed Instance Group (MIG) that disables the creation retry mode by setting the --nocreation-retries flag. is not right.\nLike above - this option doesn't Live-migrate the instances so our requirement of \"highly available including during system maintenance\" is not met.\n\nCreate an instance template with availability policy that turns on the automatic restart behaviour and sets on-host maintenance to live migrate instances during maintenance events. Deploy the application on a Managed Instance Group (MIG) based on this template. is the right option.\nEnabling automatic restart ensures that compute engine instances are automatically restarted when they crash. And Enabling \"Migrate VM Instance\" enables live migrates, i.e. compute instances are migrated during system maintenance and remain running during the migration. Automatic Restart - If your instance is set to terminate when there is a maintenance event, or if your instance crashes because of an underlying hardware issue, you can set up Compute Engine to automatically restart the instance by setting the automaticRestart field to true. This setting does not apply if the instance is taken offline through a user action, such as calling sudo shutdown, or during a zone outage.\nRef: https://cloud.google.com/compute/docs/instances/setting-instance-scheduling-options#autorestart\nEnabling the Migrate VM Instance option migrates your instance away from an infrastructure maintenance event, and your instance remains running during the migration. Your instance might experience a short period of decreased performance, although generally, most instances should not notice any difference. Live migration is ideal for instances that require constant uptime and can tolerate a short period of decreased performance.\nRef: https://cloud.google.com/compute/docs/instances/setting-instance-scheduling-options#live_migrate"
    },
    {
      "options": [
        "Deploy the application to Google Compute Engine Managed Instance Group (MIG) with autoscaling enabled based on CPU utilization.",
        "Deploy the application to Google Compute Engine Managed Instance Group (MIG). Deploy a Cloud Function to look up CPU utilization in Cloud Monitoring every minute and scale up or scale down the MIG group as needed.",
        "Deploy the application to GKE cluster with Horizontal Pod Autoscaling (HPA) enabled based on CPU utilization.",
        "Deploy the application to Google Compute Engine Managed Instance Group (MIG) with time-based autoscaling based on last months’ traffic patterns."
      ],
      "correct": "Deploy the application to Google Compute Engine Managed Instance Group (MIG) with autoscaling enabled based on CPU utilization.",
      "questionText": "Your company wants to migrate a mission-critical application to Google Cloud Platform. The application is currently hosted in your on-premises data centre and runs off several VMs. Your migration manager has suggested a “lift and shift” to Google Compute Engine Virtual Machines and has asked you to ensure the application scales quickly, automatically and efficiently based on the CPU utilization. You want to follow Google recommended practices. What should you do?",
      "explanation": "Our requirements are\n\nUse Virtual Machines directly (i.e. not container-based)\n\nScale Automatically\n\nScaling is automatic, efficient & quick\n\nDeploy the application to GKE cluster with Horizontal Pod Autoscaling (HPA) enabled based on CPU utilization. is not right.\nWe want to use virtual machines directly. A “lift and shift” from the on-premise VMs to GKE cluster is not possible. Although GKE uses virtual machines under the hood for its cluster, the autoscaling is different. It uses scaling at VMs (cluster auto-scaling) as well as at pod level (horizontal and vertical pod autoscaling). In this option, although horizontal pod autoscaling is enabled, Cluster Autoscaling isn’t, and this limits the ability to scale up.\n\nDeploy the application to Google Compute Engine Managed Instance Group (MIG) with time-based autoscaling based on last months’ traffic patterns. is not right.\nScaling based on time of the day may be insufficient especially when there is a sudden surge of requests (causing high CPU utilization) or if the requests go down suddenly (resulting in low CPU usage). We want to scale automatically, i.e. we need autoscaling solution that scales up and down based on CPU usage, which is indicative of the volume of requests processed. But, scaling based on time of the day is not indicative of the load (CPU) on the system and is therefore not right.\n\nDeploy the application to Google Compute Engine Managed Instance Group (MIG). Deploy a Cloud Function to look up CPU utilization in Cloud Monitoring every minute and scale up or scale down the MIG group as needed. is not right.\nWhile this can be done, it is not the most efficient solution when the same can be achieved more efficiently using a MIG with autoscaling based on CPU utilization.\n\nDeploy the application to Google Compute Engine Managed Instance Group (MIG) with autoscaling enabled based on CPU utilization. is the right answer.\nManaged instance groups offer autoscaling capabilities that let you automatically add or delete instances from a managed instance group based on increases or decreases in load (CPU Utilization in this case). Autoscaling helps your apps gracefully handle traffic increases and reduce costs when the need for resources is lower. You define the autoscaling policy, and the autoscaler performs automatic scaling based on the measured load (CPU Utilization in this case). Autoscaling works by adding more instances to your instance group when there is more load (upscaling), and deleting instances when the need for instances is lowered (downscaling).\nRef: https://cloud.google.com/compute/docs/autoscaler"
    },
    {
      "options": [
        "Create two external tables in BigQuery and link them to the Cloud BigTable and Cloud Storage data sources, respectively. Execute a query in BigQuery console to join up data between the two external tables for the specific gamer.",
        "Set up a Cloud Dataflow job to read data from Cloud Spanner and Cloud BigTable for the specific gamer.",
        "Set up a Cloud Dataproc Cluster to run a Hadoop job to join up data from Cloud BigTable and Cloud Storage for the specific gamer.",
        "Set up a Cloud Dataflow job to read data from Cloud Storage and Cloud BigTable for the specific gamer."
      ],
      "correct": "Create two external tables in BigQuery and link them to the Cloud BigTable and Cloud Storage data sources, respectively. Execute a query in BigQuery console to join up data between the two external tables for the specific gamer.",
      "questionText": "Your company owns a mobile game that is popular with users all over the world. The mobile game backend uses Cloud Spanner to store user state. An overnight job exports user state to a Cloud Storage bucket. The app pushes all time-series events during the game to a streaming Dataflow service that saves them to Cloud Bigtable. You are debugging an in-game issue raised by a gamer, and you want to join the user state information with data stored in Bigtable to debug. How can you do this one-off join efficiently?",
      "explanation": "We are required to join user sessions with user events efficiently. We need to look for an option that is primarily a Google service and provides this feature out of the box or with minimal configuration.\n\nSet up a Cloud Dataflow job to read data from Cloud Spanner and Cloud BigTable for the specific gamer. is not right.\nYou can make use of the Cloud Dataflow connector for Cloud Spanner (https://cloud.google.com/spanner/docs/dataflow-connector) and Dataflow Connector for Cloud Bigtable (https://cloud.google.com/bigtable/docs/hbase-dataflow-java) to retrieve data from these sources, but you can’t use Dataflow SQL to restrict this to specific users. Dataflow SQL natively only works when reading data from Pub/Sub topics, Cloud Storage file sets, and BigQuery tables.\nRef: https://cloud.google.com/dataflow/docs/guides/sql/data-sources-destinations\n\nSet up a Cloud Dataproc Cluster to run a Hadoop job to join up data from Cloud BigTable and Cloud Storage for the specific gamer. is not right.\nWhile it is certainly possible to do this using a Hadoop job, it is complicated as we would have to come up with the code/logic to extract the data and certainly not straightforward.\n\nSet up a Cloud Dataflow job to read data from Cloud Storage and Cloud BigTable for the specific gamer. is not right.\nThis option is possible, but it is not as efficient as using Big Query.\nRef: https://cloud.google.com/dataflow/docs/guides/sql/dataflow-sql-intro\n\nHere is some more documentation around this option, some of the issues are\n\nDataflow SQL expects CSV files in Cloud Storage filesets. CSV files must not contain a header row with column names; the first row in each CSV file is interpreted as a data record. - but our question doesn’t say how the exported data is stored in cloud storage.\n\nYou can only run jobs in regions that have a Dataflow regional endpoint. Our question doesn’t say which region.\nRef: https://cloud.google.com/dataflow/docs/concepts/regional-endpoints.\n\nCreating a Dataflow job can take several minutes - unlike Big Query external tables which can be created very quickly. Too many unknowns. Otherwise, this option is a good option. Here is some more information if you’d like to get a better understanding of how to use Cloud Dataflow to achieve this result. Cloud Dataflow SQL lets you use SQL queries to develop and run Dataflow jobs from the BigQuery web UI. You can join streams (such as Pub/Sub) and snapshotted datasets (such as BigQuery tables and Cloud Storage filesets); query your streams or static datasets with SQL by associating schemas with objects, such as tables, Cloud Storage filesets and Pub/Sub topics; and write your results into a BigQuery table for analysis and dashboarding.\n\nCloud Dataflow SQL supports multiple data sources including Cloud Storage and Big Query tables which are of interest for this scenario.\nRef: https://cloud.google.com/dataflow/docs/guides/sql/data-sources-destinations\n\nCreate two external tables in BigQuery and link them to the Cloud BigTable and Cloud Storage data sources, respectively. Execute a query in BigQuery console to join up data between the two external tables for the specific gamer. is the right answer.\nBig query lets you create tables that reference external data sources such as Bigtable and Cloud Storage. You can then join up these two tables through user fields and apply appropriate filters. You can achieve the result with minimal configuration using this option.\nRef: https://cloud.google.com/bigquery/external-data-sources"
    },
    {
      "options": [
        "Navigate to the APIs & Services section in GCP console and enable Cloud Pub/Sub API.",
        "Use deployment manager to configure the App Engine Application to use the specific Service Account with the necessary IAM permissions and rely on the automatic enablement of the Cloud Pub/Sub API on the first request to publish or subscribe.",
        "Grant roles/pubsub.admin IAM role to the service account and modify the application code to enable the API before publishing or subscribing.",
        "Configure the App Engine Application in GCP Console to use the specific Service Account with the necessary IAM permissions and rely on the automatic enablement of the Cloud Pub/Sub API on the first request to publish or subscribe."
      ],
      "correct": "Navigate to the APIs & Services section in GCP console and enable Cloud Pub/Sub API.",
      "questionText": "You are migrating a Python application from your on-premises data centre to Google Cloud. You want to deploy the application Google App Engine, and you modified the python application to use Cloud Pub/Sub instead of RabbitMQ. The application uses a specific service account which has the necessary permissions to publish and subscribe on Cloud Pub/Sub; however, the operations team have not enabled the Cloud Pub/Sub API yet. What should you do?",
      "explanation": "Requirements\n\nWe need to enable Cloud Pub/Sub API\n\nGet our application to use the service account.\n\nGrant roles/pubsub.admin IAM role to the service account and modify the application code to enable the API before publishing or subscribing. is not right.\nAPIs are not automatically enabled on the first connection to the service (Cloud Pub/Sub in this scenario). APIs can be enabled through Google Cloud Console, gcloud command-line and REST API.\nRef: https://cloud.google.com/service-usage/docs/enable-disable\n\nConfigure the App Engine Application in GCP Console to use the specific Service Account with the necessary IAM permissions and rely on the automatic enablement of the Cloud Pub/Sub API on the first request to publish or subscribe. is not right.\nThere is no such thing as automatic enablement of the APIs when the service (Cloud Pub/Sub in this scenario) is accessed. APIs can be enabled through Google Cloud Console, gcloud command-line and REST API.\nRef: https://cloud.google.com/service-usage/docs/enable-disable\n\nUse deployment manager to configure the App Engine Application to use the specific Service Account with the necessary IAM permissions and rely on the automatic enablement of the Cloud Pub/Sub API on the first request to publish or subscribe. is not right.\nThere is no such thing as automatic enablement of the APIs (Cloud Pub/Sub in this scenario) is accessed. APIs can be enabled through Google Cloud Console, gcloud command-line and REST API.\nRef: https://cloud.google.com/service-usage/docs/enable-disable\n\nNavigate to the APIs & Services section in GCP console and enable Cloud Pub/Sub API. is the right answer.\nFor most operational use cases, the simplest way to enable and disable services is to use the Google Cloud Console. You can create scripts; you can also use the gcloud command-line interface. If you need to program against the Service Usage API, we recommend that you use one of our provided client libraries.\nRef: https://cloud.google.com/service-usage/docs/enable-disable\nSecondly, after you create an App Engine application, the App Engine default service account is created and used as the identity of the App Engine service. The App Engine default service account is associated with your Cloud project and executes tasks on behalf of your apps running in App Engine. By default, the App Engine default service account has the Editor role in the project, so this already has the permissions to push/pull/receive messages from Cloud Pub/Sub."
    },
    {
      "options": [
        "Configure a VPN tunnel between the on-premises data centre and the GCP VPC. Create a custom route in the VPC for Google Restricted APIs IP range (199.36.153.4/30) and propagate the route over VPN. Resolve *.googleapis.com as a CNAME record to restricted.googleapis.com in your on-premises DNS server.\n",
        "Create a new VPC in GCP and deploy a proxy server like HAProxy/Squid to forward requests to Cloud Storage. Configure a VPN tunnel between the on-premises data centre and the GCP VPC. Have the servers access Cloud Storage through the proxy.",
        "Make an exception and assign public IP addresses to the servers. Configure firewall rules to allow traffic from the VM public IP addresses to the IP range of Cloud Storage.",
        "Migrate all VMs from the data centre to Google Compute Engine. Set up a Load Balancer on the GCP bucket and have the servers access Cloud Storage through the load balancer."
      ],
      "correct": "Configure a VPN tunnel between the on-premises data centre and the GCP VPC. Create a custom route in the VPC for Google Restricted APIs IP range (199.36.153.4/30) and propagate the route over VPN. Resolve *.googleapis.com as a CNAME record to restricted.googleapis.com in your on-premises DNS server.\n",
      "questionText": "Your company runs several internal applications on bare metal Kubernetes servers in your on-premises data centre. One of the applications deployed in the Kubernetes cluster uses a NAS share to save files. In preparation for the upcoming migration to Google Cloud, you want to update the application to use Google Cloud Storage instead; however, security policies prevent virtual machines from having public IP addresses. What should you do?",
      "explanation": "We need to follow Google recommended practices to achieve the result. Configuring Private Google Access for On-Premises Hosts is best achieved by VPN/Interconnect + Advertise Routes + Use restricted Google IP Range.\n\nConfigure a VPN tunnel between the on-premises data centre and the GCP VPC. Create a custom route in the VPC for Google Restricted APIs IP range (199.36.153.4/30) and propagate the route over VPN. Resolve *.googleapis.com as a CNAME record to restricted.googleapis.com in your on-premises DNS server. is the right answer, and it is what Google recommends.\nRef: https://cloud.google.com/vpc/docs/configure-private-google-access-hybrid\n“You must configure routes so that Google API traffic is forwarded through your Cloud VPN or Cloud Interconnect connection, firewall rules on your on-premises firewall to allow the outgoing traffic, and DNS so that traffic to Google APIs resolves to the IP range you've added to your routes.” “You can use Cloud Router Custom Route Advertisement to announce the Restricted Google APIs IP addresses through Cloud Router to your on-premises network. The Restricted Google APIs IP range is 199.36.153.4/30. While this is technically a public IP range, Google does not announce it publicly. This IP range is only accessible to hosts that can reach your Google Cloud projects through internal IP ranges, such as through a Cloud VPN or Cloud Interconnect connection.”"
    },
    {
      "options": [
        "Enable autohealing and set the autohealing health check to healthy (HTTP).",
        "Use a global HTTP(s) Load Balancer instead and set the load balancer health check to healthy (HTTP).",
        "Enable autoscaling on the Managed Instance Group (MIG).",
        "Use a global HTTP(s) Load Balancer instead and limit Requests Per Second (RPS) to 10."
      ],
      "correct": "Enable autohealing and set the autohealing health check to healthy (HTTP).",
      "questionText": "You deployed a Java application on four Google Cloud Compute Engine VMs in two zones behind a network load balancer. During peak usage, the application has stuck threads. This issue ultimately takes down the whole system and requires a reboot of all VMs. Your operations team have recently heard about self-healing mechanisms in Google Cloud and have asked you to identify if it is possible to automatically recreate the VMs if they remain unresponsive for 3 attempts 10 seconds apart. What should you do?",
      "explanation": "Enable autoscaling on the Managed Instance Group (MIG). is not right.\nAuto-scaling capabilities of Managed instance groups let you automatically add or delete instances from a managed instance group based on increases or decreases in load. They don't help you with re-creation should the VMs go unresponsive (unless you also enable the autohealing health checks).\nRef: https://cloud.google.com/compute/docs/autoscaler\n\nUse a global HTTP(s) Load Balancer instead and limit Requests Per Second (RPS) to 10. is not right.\nYou set RPS (Requests per Second) on load balancer when using RATE balancing mode. RPS does not affect auto-healing.\nRef: https://cloud.google.com/load-balancing/docs/https/\n\nUse a global HTTP(s) Load Balancer instead and set the load balancer health check to healthy (HTTP). is not right.\nThe health checks defined on the load balancer determine whether VM instances respond correctly to traffic. The Load balancer health checks have no impact on auto-healing. It is important to note that the health checks defined on the load balancer are different to the health checks defined on the auto-healing for managed instances group - see the explanation in the right answer for more information.\nRef: https://cloud.google.com/load-balancing/docs/health-checks#create_a_health_check\n\nEnable autohealing and set the autohealing health check to healthy (HTTP). is the right answer.\nTo enable auto-healing, you need to group the instances into a managed instance group. Managed instance groups (MIGs) maintain the high availability of your applications by proactively keeping your virtual machine (VM) instances available. An auto-healing policy on the MIG relies on an application-based health check to verify that an application is responding as expected. If the auto-healer determines that an application isn't responding, the managed instance group automatically recreates that instance.\n\nIt is essential to use separate health checks for load balancing and auto-healing. Health checks for load balancing can and should be more aggressive because these health checks determine whether an instance receives user traffic. You want to catch non-responsive instances quickly, so you can redirect traffic if necessary. In contrast, health checking for auto-healing causes Compute Engine to replace failing instances proactively, so this health check should be more conservative than a load balancing health check."
    },
    {
      "options": [
        "Create a new GCP project. Create a new App Engine Application in the new GCP project and set its region to asia-northeast-1. Delete the old App Engine application.",
        "Deploy a new app engine application in the same GCP project and set the region to asia-northeast1. Delete the old App Engine application.",
        "Update the default region property to asia-northeast1 on the App Engine Service.",
        "Update the region property to asia-northeast1 on the App Engine application."
      ],
      "correct": "Create a new GCP project. Create a new App Engine Application in the new GCP project and set its region to asia-northeast-1. Delete the old App Engine application.",
      "questionText": "You deployed a Python application to GCP App Engine Standard service in the us-central region. Most of your customers are based in Japan and are experiencing slowness due to the latency. You want to transfer the application from us-central region to asia-northeast1 region to minimize latency. What should you do?",
      "explanation": "Update the default region property to asia-northeast1 on the App Engine Service. is not right.\nApp Engine is regional, which means the infrastructure that runs your apps is located in a specific region and is managed by Google to be redundantly available across all the zones within that region. You cannot change an app's region after you set it.\nRef: https://cloud.google.com/appengine/docs/locations\n\nUpdate the region property to asia-northeast1 on the App Engine application. is not right.\nApp Engine is regional, which means the infrastructure that runs your apps is located in a specific region and is managed by Google to be redundantly available across all the zones within that region. You cannot change an app's region after you set it.\nRef: https://cloud.google.com/appengine/docs/locations\n\nDeploy a new app engine application in the same GCP project and set the region to asia-northeast1. Delete the old App Engine application. is not right.\nApp Engine is regional, and you cannot change an app's region after you set it. You can deploy additional services in the App Engine, but they will all be targeted to the same region.\nRef: https://cloud.google.com/appengine/docs/locations\n\nCreate a new GCP project. Create a new App Engine Application in the new GCP project and set its region to asia-northeast-1. Delete the old App Engine application. is the right answer.\nApp Engine is regional, and you cannot change an app's region after you set it. Therefore, the only way to have an app run in another region is by creating a new project and targeting the app engine to run in the required region (asia-northeast1 in our case).\nRef: https://cloud.google.com/appengine/docs/locations"
    },
    {
      "options": [
        "Add a label on the Cloud Storage bucket with key: Content-Type and value: application/pdf.",
        "Add a metadata tag on all the PDF file objects with key: Content-Type and value: application/pdf.",
        "Use Cloud CDN to front the static bucket and set the HTTP header displayInBrowser to 1.",
        "Modify the bucket ACLs to make all PDF files public."
      ],
      "correct": "Add a metadata tag on all the PDF file objects with key: Content-Type and value: application/pdf.",
      "questionText": "You are working for a cryptocurrency startup, and you have enabled a link to the company’s Initial Coin Offering (ICO) whitepaper on the company website – which runs off Google Cloud Storage. Your CTO clicked on this link and got prompted to save the file to their desktop. The CTO thinks this is a poor user experience and has asked you to identify if it is possible to render the file directly in the browser for all users. What should you do?",
      "explanation": "Use Cloud CDN to front the static bucket and set the HTTP header displayInBrowser to 1. is not right.\nCDN helps with caching content at the edge but doesn't help the browser in displaying pdf files.\n\nModify the bucket ACLs to make all PDF files public. is not right.\nThe fact that the browser lets users download the file suggests the browser can reach out and download the file. Sharing the PDF files publicly wouldn't make any difference.\n\nAdd a label on the Cloud Storage bucket with key: Content-Type and value: application/pdf. is not right.\nBucket labels are key: value metadata pairs that allow you to group your buckets along with other Google Cloud resources such as virtual machine instances and persistent disks. They don't determine the file's content type.\n\nAdd a metadata tag on all the PDF file objects with key: Content-Type and value: application/pdf. is the right answer.\nContent-Type allows browsers to render the object correctly. If the browser prompts users to save files to their machine, it means the browser does not see the Content-Type as application/pdf. Setting this would ensure the browser displays PDF files within the browser instead of popping up a download dialogue.\nRef: https://cloud.google.com/storage/docs/gsutil/addlhelp/WorkingWithObjectMetadata#content-type_1"
    },
    {
      "options": [
        "Run a script to generate SSH key pairs for all developers. Send an email to each developer with their private key attached. Add public keys to project-wide public SSH keys in your GCP project and configure all VM instances in the project to allow project-wide SSH keys.",
        "Share a script with the developers and ask them to run it to generate a new SSH key pair. Have them email their pubic key to you and run a script to add all the public keys to all instances in the project.",
        "Share a script with the developers and ask them to run it to generate a new SSH key pair. Have the developers add their public key to their Google Account. Ask the security administrator to grant compute.osAdminLogin role to the developers’ Google group.",
        "Run a script to generate SSH key pairs for all developers. Send an email to each developer with their private key attached. Update all VM instances in the development to add all the public keys. Have the developers present their private key to SSH to the instances."
      ],
      "correct": "Share a script with the developers and ask them to run it to generate a new SSH key pair. Have the developers add their public key to their Google Account. Ask the security administrator to grant compute.osAdminLogin role to the developers’ Google group.",
      "questionText": "You are the operations manager at your company, and you have been requested to provide administrative access to the virtual machines in the development GCP project to all members of the development team. There are over a hundred VM instances, and everyone at your company has a Google account. How can you simplify this access request while ensuring you can audit logins if needed?",
      "explanation": "Run a script to generate SSH key pairs for all developers. Send an email to each developer with their private key attached. Update all VM instances in the development to add all the public keys. Have the developers present their private key to SSH to the instances. is not right.\nSending the private keys in an email is a bad practice. Updating all VM instances to add public keys is not operationally efficient as it needs to be carried out on a per-user per-VM basis. You also need to take into consideration new user onboarding and user de-provisioning scenarios which add to the operational overhead.\n\nRun a script to generate SSH key pairs for all developers. Send an email to each developer with their private key attached. Add public keys to project-wide public SSH keys in your GCP project and configure all VM instances in the project to allow project-wide SSH keys. is not right.\nSending the private keys in an email is a bad practice. Adding public keys is not operationally efficient as it needs to be carried out on a per-user. You also need to take into consideration new user onboarding and user de-provisioning scenarios which add to the operational overhead.\n\nShare a script with the developers and ask them to run it to generate a new SSH key pair. Have them email their pubic key to you and run a script to add all the public keys to all instances in the project. is not right.\nSending the private keys in an email is a bad practice. Updating all VM instances to add public keys is not operationally efficient as it needs to be carried out on a per-user per-VM basis. You also need to take into consideration new user onboarding and user de-provisioning scenarios which add to the operational overhead.\n\nShare a script with the developers and ask them to run it to generate a new SSH key pair. Have the developers add their public key to their Google Account. Ask the security administrator to grant compute.osAdminLogin role to the developers’ Google group. is the right answer.\nBy letting users manage their SSH key pair (and it's rotation, etc.), you delete the operational burden of managing SSH keys to individual users. Secondly, granting compute.osAdminLogin role grants the group administrator permissions (as opposed to granting compute.osLogin, which does not grant administrator permissions). Finally, managing provisioning and de-provisioning is as simple as adding or removing the user from the group.\n\nOS Login lets you use Compute Engine IAM roles to manage SSH access to Linux instances efficiently and is an alternative to manually managing instance access by adding and removing SSH keys in the metadata. Before you can manage instance access using IAM roles, you must enable the OS Login feature by setting a metadata key-value pair in your project or your instance's metadata: enable-oslogin=TRUE. After you enable OS Login on one or more instances in your project, those instances accept connections only from user accounts that have the necessary IAM roles in your project or organization. There are two predefined roles.\n\nroles/compute.osLogin, which does not grant administrator permissions\n\nroles/compute.osAdminLogin, which grants administrator permissions\n\nAt any point, to revoke user access to instances that are enabled to use OS Login, remove the user roles from that user account\nRef: https://cloud.google.com/compute/docs/instances/managing-instance-access#enable_oslogin"
    },
    {
      "options": [
        "Run the query using bq with the --dry_run flag to estimate the number of bytes returned by the query. Make use of the pricing calculator to estimate the query cost.",
        "Execute the query using bq to estimate the number of rows returned by the query. Make use of the pricing calculator to estimate the query cost.",
        "Run the query using bq with the --dry_run flag to estimate the number of bytes read by the query. Make use of the pricing calculator to estimate the query cost.",
        "Switch to BigQuery flat-rate pricing. Coordinate with the analyst to run the query while on flat-rate pricing and switch back to on-demand pricing."
      ],
      "correct": "Run the query using bq with the --dry_run flag to estimate the number of bytes read by the query. Make use of the pricing calculator to estimate the query cost.",
      "questionText": "You work for a leading retail platform that enables its retailers to sell their items to over 200 million users worldwide. You persist all analytics data captured during user navigation to BigQuery. A business analyst wants to run a query to identify products that were popular with buyers in the recent thanksgiving sale. The analyst understands the query needs to iterate through billions of rows to fetch the required information but is not sure of the costs involved in the on-demand pricing model, and has asked you to help estimate the query cost. What should you do?",
      "explanation": "Switch to BigQuery flat-rate pricing. Coordinate with the analyst to run the query while on flat-rate pricing and switch back to on-demand pricing. is not right.\nThe cost of acquiring a big query slot (associated with flat-rate pricing) is significantly higher than our requirement here to run a single important query or to know how much it would cost to run that query. BigQuery offers flat-rate pricing for customers who prefer a stable monthly cost for queries rather than paying the on-demand price per TB of data processed. You enrol in flat-rate pricing by purchasing slot commitments, measured in BigQuery slots. Slot commitments start at 500 slots, and the price starts from $10000. Your queries consume this slot capacity, and you are not billed for bytes processed.\nRef: https://cloud.google.com/bigquery/pricing#flat_rate_pricing\n\nRun the query using bq with the --dry_run flag to estimate the number of bytes returned by the query. Make use of the pricing calculator to estimate the query cost. is not right.\nUnder on-demand pricing, BigQuery doesn't charge for the query execution based on the output of the query (i.e. bytes returned) but on the number of bytes processed (also referred to as bytes read or bytes scanned) to arrive at the output of the query. You are charged for the number of bytes processed whether the data is stored in BigQuery or in an external data source such as Cloud Storage, Google Drive, or Cloud Bigtable. On-demand pricing is based solely on usage. You are charged for the bytes scanned even if your query itself doesn't return any data.\nRef: https://cloud.google.com/bigquery/pricing\n\nExecute the query using bq to estimate the number of rows returned by the query. Make use of the pricing calculator to estimate the query cost. is not right.\nThis option is not as practical as identifying the number of records your query will look through (i.e. scan/process) is not straightforward. Plus BigQuery supports external data sources such as Cloud Storage, Google Drive, or Cloud Bigtable; and the developer cost associated with identifying this information from various data sources is significant, not practical and sometimes not possible.\n\nRun the query using bq with the --dry_run flag to estimate the number of bytes read by the query. Make use of the pricing calculator to estimate the query cost. is the right answer.\nBigQuery pricing is based on the number of bytes processed/read. Under on-demand pricing, BigQuery charges for queries by using one metric: the number of bytes processed (also referred to as bytes read). You are charged for the number of bytes processed whether the data is stored in BigQuery or in an external data source such as Cloud Storage, Google Drive, or Cloud Bigtable. On-demand pricing is based solely on usage.\nRef: https://cloud.google.com/bigquery/pricing"
    },
    {
      "options": [
        "Clone the GCP project and apply the deployment manager template in the new project. Review the actions in Cloud Logging and monitor for failures before applying the template in the production GCP project.",
        "Preview the changes by applying the deployment manager template with the --preview flag.",
        "Add logging statements in the deployment manager template YAML file.",
        "Apply the deployment manager template and review the actions in Cloud Logging."
      ],
      "correct": "Preview the changes by applying the deployment manager template with the --preview flag.",
      "questionText": "Your colleague updated a deployment manager template of a production application serving live traffic. You want to deploy the update to the live environment later during the night when user traffic is at its lowest. The git diff on the pull request shows the changes are substantial and you would like to review the intended changes without applying the changes in the live environment. You want to do this as efficiently and quickly as possible. What should you do?",
      "explanation": "Requirements - confirm dependencies, rapid feedback.\n\nAdd logging statements in the deployment manager template YAML file. is not right.\nDeployment Manager doesn't provide the ability to set granular logging statements. Moreover, if that were possible, the logging statements wouldn't be written to a log file until the template is applied and it is already too late as the template is applied. We haven't had a chance to confirm that the dependencies of all defined resources are adequately met.\n\nApply the deployment manager template and review the actions in Cloud Logging. is not right.\nThis option doesn't give us a chance to confirm that the dependencies of all defined resources are adequately met before executing it.\n\nClone the GCP project and apply the deployment manager template in the new project. Review the actions in Cloud Logging and monitor for failures before applying the template in the production GCP project. is not right.\nWhile we can identify whether dependencies are met by monitoring the failures, it is not rapid. We need rapid feedback on changes, and we want that before changes are committed (i.e. applied) to the project.\n\nPreview the changes by applying the deployment manager template with the --preview flag. is the right answer.\nAfter we have written a configuration file, we can preview the configuration before you create a deployment. Previewing a configuration lets you see the resources that Deployment Manager would create but does not instantiate any resources. In gcloud command-line, you use the create sub-command with the --preview flag to preview configuration changes.\nRef: https://cloud.google.com/deployment-manager"
    },
    {
      "options": [
        "Create a Kubernetes Service with type: Loadbalancer and the cloud.google.com/load-balancer-type: Internal annotation to expose the encryption endpoints running in the pods. Peer the two VPCs and have the GCE VM invoke the TCP encryption endpoints on the (Internal) Kubernetes Service DNS address.",
        "Create a Kubernetes Service with type: Loadbalancer to expose the encryption endpoints running in the pods. Configure a Cloud Armour security policy to allow traffic from GCE VM to the Kubernetes Service. Have the GCE VM invoke the TCP encryption endpoints on the Kubernetes Service DNS address.",
        "Create a Kubernetes Service with type: NodePort to expose the encryption endpoints running in the pods. Set up a custom proxy in another compute engine VM in pt-network and configure it to forward the traffic to the Kubernetes Service in the other VPC. Have the GCE VM invoke the TCP encryption endpoints on the proxy DNS address.",
        "Create a Kubernetes Service with type: Loadbalancer to expose the encryption endpoints running in the pods. Disable propagating Client IP Addresses to the pods by setting Services' .spec.externalTrafficPolicy to Cluster. Have the GCE VM invoke the TCP encryption endpoints on the Kubernetes Service DNS address."
      ],
      "correct": "Create a Kubernetes Service with type: Loadbalancer and the cloud.google.com/load-balancer-type: Internal annotation to expose the encryption endpoints running in the pods. Peer the two VPCs and have the GCE VM invoke the TCP encryption endpoints on the (Internal) Kubernetes Service DNS address.",
      "questionText": "Your team created two networks (VPC) with non-overlapping ranges in Google Cloud in the same region. The first VPC hosts an encryption service on a GKE cluster with cluster autoscaling enabled. The encryption service provides TCP endpoints to encrypt and decrypt data. The second VPC pt-network hosts a user management system on a single Google Cloud Compute Engine VM. The user management system deals with PII data and needs to invoke the encryption endpoints running on the GKE cluster to encrypt and decrypt data. What should you do to enable the compute engine VM invoke the TCP encryption endpoints while minimizing effort?",
      "explanation": "While it may be possible to set up the networking to let the compute engine instance in pt-network communicate with pods in the GKE cluster in multiple ways, we need to look for an option that minimizes effort. Generally speaking, this means using Google Cloud Platform services directly over setting up the service ourselves.\n\nCreate a Kubernetes Service with type: Loadbalancer to expose the encryption endpoints running in the pods. Disable propagating Client IP Addresses to the pods by setting Services' .spec.externalTrafficPolicy to Cluster. Have the GCE VM invoke the TCP encryption endpoints on the Kubernetes Service DNS address. is not right.\nIn GKE, services are used to expose pods to the outside world. There are multiple types of services. The three common types are - NodePort, ClusterIP, and LoadBalancer (there are two more service types - ExternalName and Headless, which are not relevant in this context). We do not want to create a Cluster IP as this is not accessible outside the cluster. And we do not want to create NodePort as this results in exposing a port on each node in the cluster; and as we have multiple replicas, this will result in them trying to open the same port on the nodes which fail. The compute engine instance in pt-network needs a single point of communication to reach GKE, and you can do this by creating a service of type LoadBalancer. The LoadBalancer service is given a public IP that is externally accessible.\nRef: https://cloud.google.com/kubernetes-engine/docs/how-to/exposing-apps\n\nexternalTrafficPolicy denotes how the service should route external traffic - including public access. Rather than trying to explain, I’ll point you to an excellent blog that does a great job of answering how this works. https://www.asykim.com/blog/deep-dive-into-kubernetes-external-traffic-policies\n\nSince we have cluster autoscaling enabled, we can have more than 1 node and possibly multiple replicas running on each node. So externalTrafficPolicy set to Cluster plays well with our requirement. Finally, we configure the compute engine to use the (externally accessible) address of the load balancer.\n\nSo this certainly looks like an option, but is it the best option that minimizes effort? One of the disadvantages of this option is that it exposes the pods publicly by using a service of type LoadBalancer. We want our compute engine to talk to the pods, but do we want to expose our pods to the whole world? Maybe not!! Let’s look at the other options to find out if there is something more relevant and secure.\n\nCreate a Kubernetes Service with type: NodePort to expose the encryption endpoints running in the pods. Set up a custom proxy in another compute engine VM in pt-network and configure it to forward the traffic to the Kubernetes Service in the other VPC. Have the GCE VM invoke the TCP encryption endpoints on the proxy DNS address. is not right.\nFor reasons explained in the above option, we don’t want to create a service of type NodePort. This service opens up a port on each node for each replica (pod). If we choose to do this, the compute engine doesn’t have a single point to contact. Instead, it would need to contact the GKE cluster nodes individually - and that is bound to have issues because we have autoscaling enabled and the nodes may scale up and scale down as per the scaling requirements. New nodes may have different IP addresses to the previous nodes, so unless the Compute engine is continuously supplied with the IP addresses of the nodes, it can’t reach them. Moreover, we have multiple replicas, and we might have multiple replicas of the pod on the same node in which case they all can’t open the same node port - once a node port is opened by one replica (pod), it can’t be used by other replicas on the same node. So this option can be ruled out without going into the rest of the answer.\n\nCreate a Kubernetes Service with type: Loadbalancer to expose the encryption endpoints running in the pods. Configure a Cloud Armour security policy to allow traffic from GCE VM to the Kubernetes Service. Have the GCE VM invoke the TCP encryption endpoints on the Kubernetes Service DNS address. is not right.\nCreating a service of type LoadBalancer and getting a Compute Engine instance to use the address of the load balancer is fine, but Cloud Armor is not required. You could use Cloud Armor to set up a whitelist policy to only let traffic through from the compute engine instance, but hang on - this option says “MIG instances”. We don’t have a managed instance group. The question mentions a single instance but not MIG. If we were to assume the single instance is part of a MIG, i.e. a MIG with a single instance, this option works too. It is more secure than the first option discussed in the explanation but at the same time more expensive. Let’s look at the other option to see if it provides a secure yet cost-effective way of achieving the same.\n\nCreate a Kubernetes Service with type: Loadbalancer and the cloud.google.com/load-balancer-type: Internal annotation to expose the encryption endpoints running in the pods. Peer the two VPCs and have the GCE VM invoke the TCP encryption endpoints on the (Internal) Kubernetes Service DNS address. is the right answer.\nCreating a service of type LoadBalancer and getting a Compute Engine instance to use the address of the load balancer is fine. We covered this previously in the first option in the explanations section. Adding the annotation cloud.google.com/load-balancer-type: Internal makes your cluster's services accessible to applications outside of your cluster that use the same VPC network and are located in the same Google Cloud region. So this improves security by not allowing public access; however, the compute engine is located in a different VPC so it can’t access.\nRef: https://cloud.google.com/kubernetes-engine/docs/how-to/internal-load-balancing\nBut peering the VPCs together enables the compute engine to access the load balancer IP. And peering is possible because they do not use overlapping IP ranges. Peering links up the two VPCs and resources inside the VPCs can communicate with each other as if they were all in a single VPC. More info about VPC peering: https://cloud.google.com/vpc/docs/vpc-peering\n\nSo this option is the right answer. It provides a secure and cost-effective way of achieving our requirements. There are several valid answers, but this option is better than the others."
    },
    {
      "options": [
        "Use HAProxy Alpine Docker images to deploy to GKE cluster. Configure HAProxy to route /dynamic/ to the Managed Instance Group (MIG) and /static/ to GCS bucket. Create a service of type LoadBalancer. Create a DNS A record on www.my-new-gcp-ace-website.com to point to the address of LoadBalancer.",
        "Configure an HTTP(s) Load Balancer for the Managed Instance Group (MIG). Configure the necessary TXT DNS records on www.my-new-gcp-ace-website.com to route requests on /dynamic/ to the Managed Instance Group (MIG) and /static/ to GCS bucket.",
        "Configure an HTTP(s) Load Balancer and configure it to route requests on /dynamic/ to the Managed Instance Group (MIG) and /static/ to GCS bucket. Create a DNS A record on www.my-new-gcp-ace-website.com to point to the address of LoadBalancer.",
        "Create a CNAME DNS record on www.my-new-gcp-ace-website.com to point to storage.googleapis.com. Configure an HTTP(s) Load Balancer for the Managed Instance Group (MIG). Set up redirection rules in Cloud Storage bucket to forward requests for non-static content to the Load Balancer address."
      ],
      "correct": "Configure an HTTP(s) Load Balancer and configure it to route requests on /dynamic/ to the Managed Instance Group (MIG) and /static/ to GCS bucket. Create a DNS A record on www.my-new-gcp-ace-website.com to point to the address of LoadBalancer.",
      "questionText": "You are hosting a new application on https://www.my-new-gcp-ace-website.com. The static content of the application is served from /static path and is hosted in a Cloud Storage bucket. The dynamic content is served from /dynamic path and is hosted on a fleet of compute engine instances belonging to a Managed Instance Group. How can you configure a single GCP Load Balancer to serve content from both paths?",
      "explanation": "As a rule of thumb, Google recommended practices mean you need to select Google Services that offer out of the box features with minimal configuration. Our requirement here is to serve content from two backends while following Google recommended practices.\n\nCreate a CNAME DNS record on www.my-new-gcp-ace-website.com to point to storage.googleapis.com. Configure an HTTP(s) Load Balancer for the Managed Instance Group (MIG). Set up redirection rules in Cloud Storage bucket to forward requests for non-static content to the Load Balancer address. is not right.\nWe can create a CNAME www.my-new-gcp-ace-website.com pointing to storage.googleapis.com; however, the cloud storage bucket does not support routing requests to a load balancer based on routing information in a file in the app folder. So this option doesn't work.\n\nUse HAProxy Alpine Docker images to deploy to GKE cluster. Configure HAProxy to route /dynamic/ to the Managed Instance Group (MIG) and /static/ to GCS bucket. Create a service of type LoadBalancer. Create a DNS A record on www.my-new-gcp-ace-website.com to point to the address of LoadBalancer. is not right.\nThis option could work, but we want to follow Google recommended practices and why deploy and manage HAProxy when there might be some other Google product that does the same with minimal configuration (there is !!)?\n\nConfigure an HTTP(s) Load Balancer for the Managed Instance Group (MIG). Configure the necessary TXT DNS records on www.my-new-gcp-ace-website.com to route requests on /dynamic/ to the Managed Instance Group (MIG) and /static/ to GCS bucket. is not right.\nTXT records are used to verify the domain and TXT records can also hold any arbitrary text, but the DNS providers don’t use the text in these TXT records for routing.\nRef: https://cloud.google.com/dns/records\nRef: https://support.google.com/cloudidentity/answer/183895?hl=en\n\nConfigure an HTTP(s) Load Balancer and configure it to route requests on /dynamic/ to the Managed Instance Group (MIG) and /static/ to GCS bucket. Create a DNS A record on www.my-new-gcp-ace-website.com to point to the address of LoadBalancer. is the right answer.\nSince we need to send requests to multiple backends, Cloud DNS can't alone help us. We need Cloud HTTPS Load Balancer - it's URL maps (a fancy name for path-based routing) helps distribute traffic to backends based on the path information.\nRef https://cloud.google.com/load-balancing/docs/url-map\nTraffic received by Cloud HTTPS Load Balancer can be configured to send all requests on /dynamic path to the MIG group; and requests on /static/ path to the bucket.\nRef Adding MIG as backend service - https://cloud.google.com/load-balancing/docs/backend-service#backend_services_and_autoscaled_managed_instance_groups.\nRef Adding a backend bucket(s) - https://cloud.google.com/load-balancing/docs/https/adding-backend-buckets-to-load-balancers\nThe Load Balancer has a public IP address. But we want to instead access on www.my-new-gcp-ace-website.com, so we configure this as an A Record in our DNS provider.\nRef: https://cloud.google.com/dns/records."
    },
    {
      "options": [
        "Save the salt value in a Kubernetes secret object. Modify the YAML configuration file to reference the secret object.",
        "Save the salt value in a Kubernetes Persistent Volume. Modify the YAML configuration file to include a Persistent Volume Claim to mount the volume and reference the password from the file.",
        "Save the salt value in a Kubernetes ConfigMap object. Modify the YAML configuration file to reference the ConfigMap object.",
        "Bake the salt value into the container image."
      ],
      "correct": "Save the salt value in a Kubernetes secret object. Modify the YAML configuration file to reference the secret object.",
      "questionText": "You’ve deployed a microservice that uses sha1 algorithm with a salt value to has usernames. You deployed this to GKE cluster using deployment file:\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: sha1_hash_app-deployment\nspec:\nselector:\n    matchLabels:\n    app: sha1_hash_app\n    replicas: 3\n    template:\n    metadata:\n        labels:\n        app: sha1_hash_app\n    spec:\n        containers:\n        - name: hash-me\n        image: gcr.io/hash-repo/sha1_hash_app:2.17\n        env:\n        - name: SALT_VALUE\n            value: \"z0rtkty12$!\"\n        ports:\n        - containerPort: 8080\n\nYou need to make changes to prevent the salt value from being stored in plain text. You want to follow Google-recommended practices. What should you do?",
      "explanation": "Bake the salt value into the container image. is not right.\nBaking passwords into Docker images is a terrible idea. Anyone who spins up a container from this image has access to the password.\n\nSave the salt value in a Kubernetes ConfigMap object. Modify the YAML configuration file to reference the ConfigMap object. is not right.\nConfigMaps are useful for storing and sharing non-sensitive, unencrypted configuration information. To use sensitive information in your clusters, you must use Secrets.\nRef: https://cloud.google.com/kubernetes-engine/docs/concepts/configmap\n\nSave the salt value in a Kubernetes Persistent Volume. Modify the YAML configuration file to include a Persistent Volume Claim to mount the volume and reference the password from the file. is not right.\nPersistent volumes should not be used for storing sensitive information. PersistentVolume resources are used to manage durable storage in a cluster, and PersistentVolumeClaim is a request for and claim to a PersistentVolume resource.\nRef: https://cloud.google.com/kubernetes-engine/docs/concepts/persistent-volumes\n\nSave the salt value in a Kubernetes secret object. Modify the YAML configuration file to reference the secret object. is the right answer.\nIn GKE, you can create a secret to hold the password; and then use the secret as an environment variable in the YAML file.\nRef: https://cloud.google.com/kubernetes-engine/docs/concepts/secret\nYou can create a secret using\n\nkubectl create secret generic passwords --from-literal sha1_hash_app_SALT_VALUE= z0rtkty12$!\n\nAnd you can then modify the YAML file to reference this secret as shown below.\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: sha1_hash_app-deployment\nspec:\nselector:\n    matchLabels:\n    app: sha1_hash_app\n    replicas: 3\n    template:\n    metadata:\n        labels:\n        app: sha1_hash_app\n    spec:\n        containers:\n        - name: hash-me\n        image: gcr.io/hash-repo/sha1_hash_app:2.17\n        env:\n        - name: SALT_VALUE\n            valueFrom:\n                    secretKeyRef:\n                        name: passwords\n                        key: sha1_hash_app_SALT_VALUE\n        ports:\n        - containerPort: 8080"
    },
    {
      "options": [
        "Set up a shared VPC across all production GCP projects and configure Cloud Monitoring dashboard on one of the projects.",
        "Create a single Stackdriver account and link all production GCP projects to it. Configure a monitoring dashboard in the Stackdriver account.",
        "Create a Stackdriver account in each project and configure all accounts to use the same service account. Create a monitoring dashboard in one of the projects.",
        "Create a Stackdriver account and a Stackdriver group in one of the production GCP projects. Add all other projects as members of the group. Configure a monitoring dashboard in the Stackdriver account."
      ],
      "correct": "Create a single Stackdriver account and link all production GCP projects to it. Configure a monitoring dashboard in the Stackdriver account.",
      "questionText": "Your company has deployed several production applications across many Google Cloud Projects. Your operations team requires a consolidated monitoring dashboard for all the projects. What should you do?",
      "explanation": "Set up a shared VPC across all production GCP projects and configure Cloud Monitoring dashboard on one of the projects. is not right.\nLinking Stackdriver to one project brings metrics from that project alone. A Shared VPC allows an organization to connect resources from multiple projects to a common Virtual Private Cloud (VPC) network so that they can communicate with each other securely and efficiently using internal IPs from that network. But it does not help in linking all projects to a single Stackdriver workspace/account.\nRef: https://cloud.google.com/vpc/docs/shared-vpc\n\nCreate a Stackdriver account in each project and configure all accounts to use the same service account. Create a monitoring dashboard in one of the projects. is not right.\nStackdriver monitoring does not use roles to gather monitoring information from the project. Instead, the Stackdriver Monitoring agent, which is a collectd-based daemon, gathers system and application metrics from virtual machine instances and sends them to Monitoring. In this case, as each project is linked to a separate Stackdriver account, it is not possible to have a consolidated view of all monitoring.\nRef: https://cloud.google.com/monitoring/agent\n\nCreate a Stackdriver account and a Stackdriver group in one of the production GCP projects. Add all other projects as members of the group. Configure a monitoring dashboard in the Stackdriver account. is not right.\nAs the other projects are not linked to the stack driver, they can't be monitored. Moreover, you can not add projects to Stackdriver groups. Groups provide a mechanism for alerting on the behaviour of a set of resources, rather than on individual resources. For example, you can create an alerting policy that is triggered if some number of resources in the group violates a particular condition (for example, CPU load), rather than having each resource inform you of violations individually.\nRef: https://cloud.google.com/monitoring/groups\n\nCreate a single Stackdriver account and link all production GCP projects to it. Configure a monitoring dashboard in the Stackdriver account. is the right answer.\nYou can monitor resources of different projects in a single Stackdriver account by creating a Stackdriver workspace. A Stackdriver workspace is a tool for monitoring resources contained in one or more Google Cloud projects or AWS accounts. Each Workspace can have between 1 and 100 monitored projects, including Google Cloud projects and AWS accounts. A Workspace accesses metric data from its monitored projects, but the metric data and log entries remain in the individual projects.\nRef: https://cloud.google.com/monitoring/workspaces"
    },
    {
      "options": [
        "Run the batch jobs in a non-preemptible shared core compute engine instance that supports short periods of bursting.",
        "Run the batch jobs in a preemptible compute engine instance of appropriate machine type.",
        "Run the batch jobs in a GKE cluster on a node pool with four instances of type f1-micro.",
        "Run the batch jobs in a GKE cluster on a node pool with a single instance of type e2-small."
      ],
      "correct": "Run the batch jobs in a preemptible compute engine instance of appropriate machine type.",
      "questionText": "An application that you are migrating to Google Cloud relies on overnight batch jobs that take between 2 to 3 hours to complete. You want to do this at a minimal cost. Where should you run these batch jobs?",
      "explanation": "Requirements - achieve end goal while minimizing service costs.\n\nRun the batch jobs in a GKE cluster on a node pool with a single instance of type e2-small. is not right.\nWe do not know if a small instance is capable of handling all the batch volume. Plus this is not the most cost-effective of the options.\n\nRun the batch jobs in a GKE cluster on a node pool with four instances of type f1-micro. is not right.\nWe do not know if four micro instances are capable of handling all the batch volume. Plus this is not the most cost-effective of the options.\n\nRun the batch jobs in a non-preemptible shared core compute engine instance that supports short periods of bursting. is not right.\nWe can use an instance that supports micro bursting, but we have a job that runs for 2 hours. Bursting is suitable for short periods.\n\nRun the batch jobs in a preemptible compute engine instance of appropriate machine type. is the right answer.\nWe minimize the cost by selecting a preemptible instance of the appropriate type. If the preemptible instance is terminated, the next nightly run picks up the unprocessed volume."
    },
    {
      "options": [
        "Initialize the logging pod during the GKE Cluster creation.",
        "Deploy the logging pod in a DaemonSet Kubernetes object.",
        "Deploy the logging pod in a StatefulSet Kubernetes object.",
        "Add the logging pod in the Deployment YAML file."
      ],
      "correct": "Deploy the logging pod in a DaemonSet Kubernetes object.",
      "questionText": "You are exploring the possibility of migrating a mission-critical application from your on-premises data centre to Google Cloud Platform. You want to host this on a GKE cluster with autoscaling enabled, and you need to ensure each node can run a pod to push the application logs to a third-party logging platform. How should you deploy the pod?",
      "explanation": "Add the logging pod in the Deployment YAML file. is not right.\nIn our scenario, we need just one instance of the monitoring pod running on each node. Bundling the monitoring pod with a deployment object may result in multiple pod instances on the same node. In GKE, deployments represent a set of multiple, identical Pods with no unique identities. Deployment runs multiple replicas of your application and automatically replaces any instances that fail or become unresponsive. In this way, Deployments help ensure that one or more instances of your application are available to serve user requests.\nhttps://cloud.google.com/kubernetes-engine/docs/concepts/deployment\n\nInitialize the logging pod during the GKE Cluster creation. is not right.\nYou can not use gcloud init to initialize a monitoring pod. gcloud initializer performs the following setup steps.\n\nAuthorizes gcloud and other SDK tools to access Google Cloud Platform using your user account credentials, or from an account of your choosing whose credentials are already available.\n\nSets up a new or existing configuration.\n\nSets properties in that configuration, including the current project and optionally, the default Google Compute Engine region and zone you'd like to use.\nRef: https://cloud.google.com/sdk/gcloud/reference/init\n\nDeploy the logging pod in a StatefulSet Kubernetes object. is not right.\nIn GKE, StatefulSets represents a set of Pods with unique, persistent identities and stable hostnames that GKE maintains regardless of where they are scheduled. The state information and other resilient data for any given StatefulSet Pod are maintained in persistent disk storage associated with the StatefulSet. The primary purpose of StatefulSets is to set up persistent storage for pods that are deployed across multiple zones.\nRef: https://cloud.google.com/kubernetes-engine/docs/concepts/statefulset\nAlthough persistent volumes can be used, they are limited to two zones, and you'd have to get into node affinity if you want to use a persistent volume with a pod on a zone that is not covered by the persistent volumes zones.\nRef: https://kubernetes.io/docs/setup/best-practices/multiple-zones/\n\nDeploy the logging pod in a DaemonSet Kubernetes object. is the right answer.\nIn GKE, DaemonSets manage groups of replicated Pods and adhere to a one-Pod-per-node model, either across the entire cluster or a subset of nodes. As you add nodes to a node pool, DaemonSets automatically add Pods to the new nodes as needed. So, this is a perfect fit for our monitoring pod.\nRef: https://cloud.google.com/kubernetes-engine/docs/concepts/daemonset\nDaemonSets are useful for deploying ongoing background tasks that you need to run on all or certain nodes, and which do not require user intervention. Examples of such tasks include storage daemons like ceph, log collection daemons like fluentd, and node monitoring daemons like collectd. For example, you could have DaemonSets for each type of daemon run on all of your nodes. Alternatively, you could run multiple DaemonSets for a single type of daemon, but have them use different configurations for different hardware types and resource needs."
    },
    {
      "options": [
        "Use 172.16.0.0/12 CIDR range.",
        "Use 10.0.0.0/8 CIDR range.",
        "Use 0.0.0.0/0 CIDR range.",
        "Use 192.168.0.0/16 CIDR range."
      ],
      "correct": "Use 10.0.0.0/8 CIDR range.",
      "questionText": "The application development team at your company wants to use the biggest CIDR range possible for a VPC and has asked for your suggestion. Your operations team is averse to using any beta features. What should you suggest?",
      "explanation": "Use 10.0.0.0/8 CIDR range. is the right answer.\nThe private network range is defined by IETF (Ref: https://tools.ietf.org/html/rfc1918) and adhered to by all cloud providers. The supported internal IP Address ranges are\n\n24-bit block 10.0.0.0/8 (16777216 IP Addresses)\n\n20-bit block 172.16.0.0/12 (1048576 IP Addresses)\n\n16-bit block 192.168.0.0/16 (65536 IP Addresses)\n\n10.0.0.0/8 gives you the most extensive range - 16777216 IP Addresses."
    },
    {
      "options": [
        "1. Create a new Cloud DNS zone and set its visibility to private. 2. When provisioning the VMs, associate the DNS records with the new DNS zone.",
        "1. Provision the VMs with custom hostnames.",
        "1. Create a new Cloud DNS zone and a new VPC and associate the DNS zone with the VPC. 2. When provisioning the VMs, associate the DNS records with the new DNS zone. 3. Configure firewall rules to block all external (public) traffic. 4. Finally, configure the DNS zone associated with the default VPC to direct all requests to the new DNS zone.",
        "1. Install a new BIND DNS server on Google Compute Engine, using the BIND name server software (BIND9). 2. Configure a Cloud DNS forwarding zone to direct all requests to the Internal BIND DNS server. 3. When provisioning the VMs, associate the DNS records with the Internal BIND DNS server."
      ],
      "correct": "1. Create a new Cloud DNS zone and set its visibility to private. 2. When provisioning the VMs, associate the DNS records with the new DNS zone.",
      "questionText": "You migrated an internal HR system from an on-premises database to Google Cloud Compute Engine Managed Instance Group (MIG). The networks team at your company has asked you to associate the internal DNS records of the VMs with a custom DNS zone. You want to follow Google recommended practices. What should you do?",
      "explanation": "Our requirements here are\n\nInternal, and\n\nCustom Zone\n\n1. Provision the VMs with custom hostnames. is not right.\nThis option doesn’t create the DNS records in a custom DNS zone.\n\n1. Install a new BIND DNS server on Google Compute Engine, using the BIND name server software (BIND9).\n2. Configure a Cloud DNS forwarding zone to direct all requests to the Internal BIND DNS server.\n3. When provisioning the VMs, associate the DNS records with the Internal BIND DNS server. is not right.\nThis option might be possible but not something Google recommends. The Cloud DNS service offering from Google already offers these features, so it is pointless installing a custom DNS server to do that.\n\n1. Create a new Cloud DNS zone and a new VPC and associate the DNS zone with the VPC.\n2. When provisioning the VMs, associate the DNS records with the new DNS zone.\n3. Configure firewall rules to block all external (public) traffic.\n4. Finally, configure the DNS zone associated with the default VPC to direct all requests to the new DNS zone. is not right.\nThis doesn’t make any sense. Moreover, the two VPCs can’t communicate without VPC peering.\nRef: https://cloud.google.com/dns/docs/overview#concepts\n\n1. Create a new Cloud DNS zone and set its visibility to private.\n2. When provisioning the VMs, associate the DNS records with the new DNS zone. is the right answer.\nYou should do when you want internal DNS records in a custom zone. Cloud DNS gives you the option of private zones and internal DNS names.\nRef: https://cloud.google.com/dns/docs/overview#concepts"
    },
    {
      "options": [
        "Create a new GCP project for the performance testing environment using gcloud and copy the application from the development GCP project into the performance testing GCP project.",
        "Use gcloud to deploy the application to a new performance testing GCP project by specifying the --project parameter. Select Yes when prompted for confirmation on creating a new project.",
        "Configure a Deployment Manager YAML template to copy the application from the development GCP project into the performance testing GCP project.",
        "Create a new GCP project for the performance testing environment using gcloud and deploy your App Engine application to the new GCP project."
      ],
      "correct": "Create a new GCP project for the performance testing environment using gcloud and deploy your App Engine application to the new GCP project.",
      "questionText": "You are developing a simple application in App Engine Standard service. Unit testing and user acceptance testing has succeeded, and you want to build a new App Engine application to serve as your performance testing environment. What should you do?",
      "explanation": "Create a new GCP project for the performance testing environment using gcloud and copy the application from the development GCP project into the performance testing GCP project. is not right.\nYou can use gcloud to create a new project, but you can not copy a deployed application from one project to another. Google App Engine doesn’t offer this feature.\n\nConfigure a Deployment Manager YAML template to copy the application from the development GCP project into the performance testing GCP project. is not right.\nThe deployment manager configuration file contains configuration about the resources that need to be created in Google cloud; however, it does not offer the feature to copy app engine deployment into a new project.\n\nUse gcloud to deploy the application to a new performance testing GCP project by specifying the --project parameter. Select Yes when prompted for confirmation on creating a new project. is not right.\nYou can deploy using gcloud app deploy and target it to a different project using --project flag. However, you can only deploy to an existing project as the gcloud app deploy command is unable to create a new project if it doesn't already exist.\n\nCreate a new GCP project for the performance testing environment using gcloud and deploy your App Engine application to the new GCP project. is the right answer.\nYou can deploy to a different project by using --project flag. By default, the service is deployed the current project configured via:\n\n$ gcloud config set core/project PROJECT\n\nTo override this value for a single deployment, use the --project flag:\n\n$ gcloud app deploy ~/my_app/app.yaml --project=PROJECT\n\nRef: https://cloud.google.com/sdk/gcloud/reference/app/deploy"
    },
    {
      "options": [
        "Grant the data custodian team Storage Object Creator IAM role.",
        "Grant the data custodian team Storage Admin IAM role.",
        "Grant the data custodian team Project Editor IAM role.",
        "Grant the data custodian team Storage Object Admin IAM role."
      ],
      "correct": "Grant the data custodian team Storage Admin IAM role.",
      "questionText": "A recent reorganization in your company has seen the creation of a new data custodian team – responsible for managing data in all storage locations. Your production GCP project uses buckets in Cloud Storage, and you need to delegate control to the new team to manage objects and buckets in your GCP project. What role should you grant them?",
      "explanation": "Grant the data custodian team Project Editor IAM role. is not right.\nThe project editor is a primitive role that grants a lot more than what we need here. Google doesn't recommend using Primitive roles.\nRef: https://cloud.google.com/iam/docs/understanding-roles#primitive_role_definitions\nThe project editor role provides all viewer permissions, plus permissions for actions that modify state, such as changing existing resources.\n\nGrant the data custodian team Storage Object Admin IAM role. is not right.\nWhile this role grants full access to the objects, it does not grant access to the buckets so users of this role can not \"manage buckets\". This role grants full control over objects, including listing, creating, viewing, and deleting objects.\nRef: https://cloud.google.com/iam/docs/understanding-roles#storage-roles\n\nGrant the data custodian team Storage Object Creator IAM role. is not right.\nThis role allows users to create objects. It does not permit to view, delete, or overwrite objects.\nRef: https://cloud.google.com/iam/docs/understanding-roles#storage-roles\n\nGrant the data custodian team Storage Admin IAM role. is the right answer.\nThis role grants full control of buckets and objects. When applied to an individual bucket, control applies only to the specified bucket and objects within the bucket.\nRef: https://cloud.google.com/iam/docs/understanding-roles#storage-roles"
    },
    {
      "options": [
        "Convert the application into a set of functions and deploy them in Google Cloud Functions.",
        "Deploy the containerized version of the application in Cloud Run on GKE.",
        "Deploy the containerized version of the application in Cloud Run.",
        "Deploy the containerized version of the application in App Engine Flex.",
        "Deploy the containerized version of the application in Google Kubernetes Engine (GKE)."
      ],
      "correct": "Deploy the containerized version of the application in Cloud Run on GKE.",
      "questionText": "Your team is responsible for the migration of all legacy on-premises applications to Google Cloud. Your team is a big admirer of serverless and has chosen App Engine Standard as the preferred choice for compute workloads. Your manager asked you to migrate a legacy accounting application built in C++, but you realized App Engine Standard doesn’t support C++. What GCP compute services should you use instead to maintain the serverless aspect? (Choose two answers)",
      "explanation": "App engine standard currently supports Python, Java, Node.js, PHP, Ruby and Go.\nRef: https://cloud.google.com/appengine/docs/standard/\nThe question already states App Engine doesn’t support C#. We are required to ensure we maintain the serverless aspect of our application.\n\nConvert the application into a set of functions and deploy them in Google Cloud Functions. is not right.\nCloud Functions is a serverless platform where you can run the code in the cloud without having to provision servers. You split your application functionality into multiple functions, and each of these is defined as a cloud function. Cloud Functions don’t support C#. Supported runtimes are Python, Node.js and Go.\nRef: https://cloud.google.com/functions\n\nDeploy the containerized version of the application in App Engine Flex. is not right.\nWhile App Engine flexible lets us customize runtimes or provide our runtime by supplying a custom Docker image or Dockerfile from the open-source community, it uses compute engine virtual machines, so it is not serverless.\nRef: https://cloud.google.com/appengine/docs/flexible/\n\nDeploy the containerized version of the application in Google Kubernetes Engine (GKE). is not right.\nGKE, i.e. Google Kubernetes Clusters uses compute engine virtual machines, so it is not serverless.\nRef: https://cloud.google.com/kubernetes-engine\n\nDeploy the containerized version of the application in Cloud Run. is the right answer.\nCloud Run is a fully managed compute platform that automatically scales your stateless containers. Cloud Run is serverless: it abstracts away all infrastructure management, so you can focus on what matters most—building great applications. Run your containers in fully managed Cloud Run or on Anthos, which supports both Google Cloud and on‐premises environments. Cloud Run is built upon an open standard, Knative, enabling the portability of your applications.\nRef: https://cloud.google.com/run\n\nDeploy the containerized version of the application in Cloud Run on GKE. is the right answer.\nCloud Run implements the Knative serving API, an open-source project to run serverless workloads on top of Kubernetes. That means you can deploy Cloud Run services anywhere Kubernetes runs. And suppose you need more control over your services (like access to GPU or more memory). In that case, you can also deploy these serverless containers in your GKE cluster instead of using the fully managed environment. When using the fully managed environment, Cloud Run on GKE is serverless.\nRef: https://github.com/knative/serving/blob/master/docs/spec/spec.md\nRef: https://cloud.google.com/blog/products/serverless/cloud-run-bringing-serverless-to-containers"
    },
    {
      "options": [
        "Review the project settings in the App Engine application configuration files.",
        "Review the properties of the active gcloud configurations by executing gcloud config list.",
        "Review the project settings in the App Engine deployment YAML file.",
        "Review the project settings in the Deployment Manager console."
      ],
      "correct": "Review the properties of the active gcloud configurations by executing gcloud config list.",
      "questionText": "Your operations team have deployed an update to a production application running in Google Cloud App Engine Standard service. The deployment was successful, but your operations are unable to find this deployment in the production GCP project. What should you do?",
      "explanation": "Review the project settings in the App Engine deployment YAML file. is not right.\nThe Yaml file of application does not hold Google project information.\n\nReview the project settings in the App Engine application configuration files. is not right.\nThe web application file of the application does not hold Google project information.\n\nReview the project settings in the Deployment Manager console. is not right.\nGoogle Cloud Deployment Manager allows you to specify all the resources needed for your application in a declarative format using YAML. In this scenario, we haven't used the Cloud Deployment Manager to deploy. The app was deployed using gcloud app deploy so this option is not right.\nRef: https://cloud.google.com/deployment-manager\n\nReview the properties of the active gcloud configurations by executing gcloud config list. is the right answer.\nIf the deployment was successful, but it did not deploy to the intended project, the application would have been deployed to a different project. In the same gcloud shell, you can identify the current properties of the configuration by executing gcloud config list. The output returns config properties such as project, account, etc., as well as app-specific properties such as app/promote_by_default, app/stop_previous_version.\nRef: https://cloud.google.com/sdk/gcloud/reference/config/list"
    },
    {
      "options": [
        "Ask your security administrator to grant you the Billing Account Administrator role on the existing Billing Account. Create new development and test projects and link them to the existing Billing Account.",
        "Ask your security administrator to grant you the Billing Account Administrator role on the existing Billing Account. Link all development and test projects to the existing Billing Account.",
        "Ask your security administrator to grant you the Billing Account Creator role on the GCP organization and Project Billing Manager role on all the development and test projects. Link all the development and test projects to an existing Billing Account.",
        "Ask your security administrator to grant you the Billing Account Creator role on the GCP organization and Project Billing Manager role on all the development and test projects. Create a new Billing Account and link all the development and test projects to the new Billing Account."
      ],
      "correct": "Ask your security administrator to grant you the Billing Account Creator role on the GCP organization and Project Billing Manager role on all the development and test projects. Create a new Billing Account and link all the development and test projects to the new Billing Account.",
      "questionText": "Your finance department wants you to create a new billing account and link all development and test Google Cloud Projects to the new billing account. What should you do?",
      "explanation": "We are required to link an existing google cloud project with a new billing account.\n\nAsk your security administrator to grant you the Billing Account Administrator role on the existing Billing Account. Create new development and test projects and link them to the existing Billing Account. is not right.\nWe do not need to create new projects.\n\nAsk your security administrator to grant you the Billing Account Creator role on the GCP organization and Project Billing Manager role on all the development and test projects. Link all the development and test projects to an existing Billing Account. is not right.\nWe want to link the projects with a new billing account, so this option is not right.\n\nAsk your security administrator to grant you the Billing Account Administrator role on the existing Billing Account. Link all development and test projects to the existing Billing Account. is not right.\nWe want to link the projects with a new billing account, so this option is not right.\n\nAsk your security administrator to grant you the Billing Account Creator role on the GCP organization and Project Billing Manager role on all the development and test projects. Create a new Billing Account and link all the development and test projects to the new Billing Account. is the right answer.\nThe purpose of the Project Billing Manager is to Link/unlink the project to/from a billing account. It is granted at the organization or project level. Project Billing Manager role allows a user to attach the project to the billing account, but does not grant any rights over resources. Project Owners can use this role to allow someone else to manage the billing for the project without granting them resource access.\n\nBilling Account Creator - Use this role for initial billing setup or to allow the creation of additional billing accounts.\nRef: https://cloud.google.com/billing/docs/how-to/billing-access"
    },
    {
      "options": [
        "Grant the operations team roles/spanner.database.reader IAM role.",
        "Grant the operations team roles/stackdriver.accounts.viewer IAM role.",
        "Grant the operations team roles/monitoring.viewer IAM role.",
        "Grant the operations team roles/spanner.database.user IAM role."
      ],
      "correct": "Grant the operations team roles/monitoring.viewer IAM role.",
      "questionText": "Your company owns a mobile game that is popular with users all over the world. The mobile game backend uses Cloud Spanner to store user state. An overnight job exports user state to a Cloud Storage bucket. Your operations team needs access to monitor the spanner instance but not have the permissions to view or edit user data. What IAM role should you grant the operations team?",
      "explanation": "Requirements -\n\nMonitoring access but no data access\n\nStreamlined solution\n\nGoogle recommended practices (i.e. look for something out of the box).\n\nGrant the operations team roles/spanner.database.reader IAM role. is not right.\nroles/spanner.databaseReader provides permission to read from the Spanner database, execute SQL queries on the database, and view the schema. This role provides read access to data.\n\nGrant the operations team roles/spanner.database.user IAM role. is not right.\nroles/spanner.databaseUser provides permission to read from and write to the Spanner database, execute SQL queries on the database, and view and update the schema. This role provides both read and write access to data.\n\nGrant the operations team roles/stackdriver.accounts.viewer IAM role. is not right.\nroles/stackdriver.accounts.viewer read-only access to get and list information about Stackdriver account structure. Thie role does not provide monitor access to Cloud Spanner.\n\nGrant the operations team roles/monitoring.viewer IAM role. is the right answer.\nroles/monitoring.viewer provides read-only access to get and list information about all monitoring data and configurations. This role provides monitoring access and fits our requirements.\nRef: https://cloud.google.com/iam/docs/understanding-roles#cloud-spanner-roles"
    },
    {
      "options": [
        "Grant the auditors’ group roles/logging.viewer and roles/bigquery.dataViewer IAM roles.",
        "Grant the auditors’ user accounts roles/logging.viewer and roles/bigquery.dataViewer IAM roles.",
        "Grant the auditors’ group custom IAM roles with specific permissions.",
        "Grant the auditors’ user accounts custom IAM roles with specific permissions."
      ],
      "correct": "Grant the auditors’ group roles/logging.viewer and roles/bigquery.dataViewer IAM roles.",
      "questionText": "Your company retains all its audit logs in BigQuery for 10 years. At the annual audit every year, you need to provide the auditors' access to the audit logs. You want to follow Google recommended practices. What should you do?",
      "explanation": "Grant the auditors’ user accounts roles/logging.viewer and roles/bigquery.dataViewer IAM roles. is not right.\nSince auditing happens several times a year, we don't want to repeat the process of granting multiple roles to multiple users every time. Instead, we want to define a group with the required grants (a one time task) and assign this group to the auditor users during the time of the audit.\n\nGrant the auditors’ user accounts custom IAM roles with specific permissions. is not right.\nGoogle already provides roles that fit the external auditing requirements, so we don't need to create custom roles. Nothing stops us from creating custom IAM roles to achieve the same purpose, but this doesn't follow \"Google-recommended practices.\"\n\nGrant the auditors’ group custom IAM roles with specific permissions. is not right.\nGoogle already provides roles that fit the external auditing requirements, so we don't need to create custom roles. Nothing stops us from creating custom IAM roles to achieve the same purpose, but this doesn't follow \"Google-recommended practices.\"\n\nGrant the auditors’ group roles/logging.viewer and roles/bigquery.dataViewer IAM roles. is the right answer.\nFor external auditors, Google recommends we grant logging.viewer and bigquery.dataViewer roles. Since auditing happens several times a year to review the organization's audit logs, it is recommended we create a group with these grants and assign the group to auditor user accounts during the time of the audit.\nRef: https://cloud.google.com/iam/docs/roles-audit-logging#scenario_external_auditors"
    },
    {
      "options": [
        "Create a new IAM service account with the access scope cloud-platform and configure the script to use this service account.",
        "Create a new IAM service account with the access scope devstorage.write_only and configure the script to use this service account.",
        "Grant roles/storage.objectCreator IAM role to the service account used by the VM.",
        "Grant roles/storage.objectAdmin IAM role to the service account used by the VM."
      ],
      "correct": "Grant roles/storage.objectCreator IAM role to the service account used by the VM.",
      "questionText": "Your manager asked you to write a script to upload objects to a Cloud Storage bucket. How should you set up the IAM access to enable the script running in a Google Compute VM upload objects to Cloud Storage?",
      "explanation": "Our requirements are\n\nGoogle recommended practices\n\nMultiple compute engine instances to write data to a bucket.\n\nCreate a new IAM service account with the access scope devstorage.write_only and configure the script to use this service account. is not right.\nYou can't attach scope when creating a service account.\nRef: https://cloud.google.com/sdk/gcloud/reference/iam/service-accounts/create\n\nCreate a new IAM service account with the access scope cloud-platform and configure the script to use this service account. is not right.\nYou can't attach scope when creating a service account.\nRef: https://cloud.google.com/sdk/gcloud/reference/iam/service-accounts/create\n\nGrant roles/storage.objectAdmin IAM role to the service account used by the VM. is not right.\nYou need to provide Compute Engine instances permissions to write data into a particular Cloud Storage bucket. Storage Object Admin (roles/storage.objectAdmin) grants full control over objects, including listing, creating, viewing, and deleting objects. Granting this role goes against the principle of least privilege.\nRef: https://cloud.google.com/storage/docs/access-control/iam-roles\n\nGrant roles/storage.objectCreator IAM role to the service account used by the VM. is the right answer.\nYou need to provide Compute Engine instances permissions to write data into a particular Cloud Storage bucket. Storage Object Creator (roles/storage.objectCreator) allows users to create objects. Does not permit to view, delete, or overwrite objects. This permission is what the script needs to write data to the bucket. So we create a service account, add this IAM role and let the compute engine instances use this service account to write objects to the bucket.\nRef: https://cloud.google.com/storage/docs/access-control/iam-roles"
    },
    {
      "options": [
        "Set the service account in the Identity and API access section when provisioning the compute engine VM.",
        "Execute gcloud iam service-accounts keys create to generate a JSON key for the service account. Copy the contents of JSON key to ~/.identity/default-service-account.json overwrite the default service account.",
        "Execute gcloud iam service-accounts keys create to generate a JSON key for the service account. Add a metadata tag on the GCP project with key: service-account and value: <contents of JSON key file>.",
        "Execute gcloud iam service-accounts keys create to generate a JSON key for the service account. Add a metadata tag to the compute instance with key: service-account and value: <contents of JSON key file>."
      ],
      "correct": "Set the service account in the Identity and API access section when provisioning the compute engine VM.",
      "questionText": "You are enhancing a production application currently running on an Ubuntu Linux VM on Google Compute Engine. The new enhancements require a connection to Cloud SQL to persist user addresses. Your colleague has created the Cloud SQL instance and an IAM service account with the correct permissions but doesn’t know how to configure the VM to use this service account, and has asked for your assistance. What should you do?",
      "explanation": "Set the service account in the Identity and API access section when provisioning the compute engine VM. is the right answer.\nYou can set the service account at the time of creating the compute instance. You can also update the service account used by the instance - this requires that you stop the instance first and then update the service account. Setting/Updating the service account can be done either via the web console or by executing gcloud command or by the REST API. See below an example for updating the service account through gcloud command.\n\ngcloud compute instances set-service-account instance-1 --zone=us-central1-a --service-account=my-new-service-account@gcloud-gcp-ace-lab-266520.iam.gserviceaccount.com\nUpdated [https://www.googleapis.com/compute/v1/projects/gcloud-gcp-ace-lab-266520/zones/us-central1-a/instances/instance-1].\n\nExecute gcloud iam service-accounts keys create to generate a JSON key for the service account. Add a metadata tag on the GCP project with key: service-account and value: {contents of JSON key file}. is not right.\nWhile updating the service account for a compute instance can be done through the console, gcloud or the REST API, they don't do it based on the JSON Private Key.\n\nExecute gcloud iam service-accounts keys create to generate a JSON key for the service account. Add a metadata tag to the compute instance with key: service-account and value: {contents of JSON key file}. is not right.\nSetting the metadata tag does not force the compute engine instance to use the specific service account.\n\nExecute gcloud iam service-accounts keys create to generate a JSON key for the service account. Copy the contents of JSON key to ~/.identity/default-service-account.json overwrite the default service account. is not right.\nYou can configure a VM to use a specific service account by providing the relevant JSON credentials file, but the procedure is different. Copying the JSON file to a specific path alone is not sufficient. Moreover, the path mentioned is wrong. See below for a use case where a VM which is unable to list cloud storage buckets is updated to use a service account, and it can then list the buckets. Before using a specific service account, execute gsutil ls to list buckets, and it fails.\n\n$ gsutil ls\nServiceException: 401 Anonymous caller does not have storage.buckets.list access to project 393066724129.\n\nWithin the VM, execute the command below to use the service account. (Assumes that you have created a service account that provides the necessary permissions and has copied it over the VM)\n\ngcloud auth activate-service-account admin-service-account@gcloud-gcp-ace-266520.iam.gserviceaccount.com --key-file=~/compute-engine-service-account.json\nActivated service account credentials for: [admin-service-account@gcloud-gcp-ace-266520.iam.gserviceaccount.com]\n\nThe output above doesn't show this, but the credentials are written to the file.\n\n/home/gcloud_gcp_ace_user/.config/gcloud/legacy_credentials/admin-service-account@gcloud-gcp-ace-266520.iam.gserviceaccount.com/adc.json\n\nNow, use gsutil ls again to list buckets, and it works.\n\n$ gsutil ls\ngs://test-gcloud-gcp-ace-2020-bucket-1/\ngs://test-gcloud-gcp-ace-2020-bucket-2/"
    },
    {
      "options": [
        "Add a new index to Cloud Datastore instance in the development project by running gcloud datastore indexes create and modify your application on your workstation to retrieve the data from Cloud Datastore using the index.",
        "Initiate an export of Cloud Datastore instance from development GCP project by executing gcloud datastore export. Modify your applications to point to the export.",
        "Install Datastore emulator to provide local emulation of the production datastore environment in your local workstation by running apt get install.",
        "Install Datastore emulator to provide local emulation of the production datastore environment in your local workstation by running gcloud components install."
      ],
      "correct": "Install Datastore emulator to provide local emulation of the production datastore environment in your local workstation by running gcloud components install.",
      "questionText": "You are developing a mobile game that uses Cloud Datastore for gaming leaderboards and player profiles. You want to test an aspect of this solution locally on your Ubuntu workstation which already has Cloud SDK installed. What should you do?",
      "explanation": "Requirements - test your application locally.\n\nInitiate an export of Cloud Datastore instance from development GCP project by executing gcloud datastore export. Modify your applications to point to the export. is not right.\nBy all means, you can export a copy of all or a subset of entities from Google Cloud Datastore to another storage system such as Google Cloud Storage. But, the application is configured to connect to a Cloud Datastore instance, not another system that stores a raw dump of exported data. So this option is not right.\n\nAdd a new index to Cloud Datastore instance in the development project by running gcloud datastore indexes create and modify your application on your workstation to retrieve the data from Cloud Datastore using the index. is not right.\nYou could create an index, but this doesn't help your application emulate connections to Cloud Datastore on your laptop. So this option is not right.\n\nInstall Datastore emulator to provide local emulation of the production datastore environment in your local workstation by running apt get install. is not right.\nDatastore emulator is a gcloud component, and you can't install gcloud components using apt get. So this option is not right.\n\nInstall Datastore emulator to provide local emulation of the production datastore environment in your local workstation by running gcloud components install. is the right answer.\nThe Datastore emulator provides local emulation of the production Datastore environment. You can use the emulator to develop and test your application locally.\nRef: https://cloud.google.com/datastore/docs/tools/datastore-emulator"
    },
    {
      "options": [
        "Execute gcloud configurations activate [config name] to activate the configuration for each project and execute gcloud config list to create and start the VM.",
        "Execute gcloud config configuration create [config name] to create two configurations, one for each project. Execute gcloud configurations list to create and start the VMs.",
        "Execute gcloud config configuration create [config name] to create two configurations, one for each project. Execute gcloud config configurations activate [config name] to activate the first configuration, and gcloud compute instances create to create the VM. Repeat the steps for other configuration.",
        "Execute gcloud configurations activate [config name] to activate the configuration for each project and execute gcloud configurations list to create and start the VM."
      ],
      "correct": "Execute gcloud config configuration create [config name] to create two configurations, one for each project. Execute gcloud config configurations activate [config name] to activate the first configuration, and gcloud compute instances create to create the VM. Repeat the steps for other configuration.",
      "questionText": "You have one GCP project with default region and zone set to us-east1 and us-east1-b respectively. You have another GCP project with default region and zone set to us-west1 and us-west1-a respectively. You want to provision a VM in each of these projects efficiently using gcloud CLI. What should you do?",
      "explanation": "Execute gcloud config configuration create [config name] to create two configurations, one for each project. Execute gcloud configurations list to create and start the VMs. is not right.\ngcloud configurations list is an invalid command. To list the existing named configurations, you need to execute gcloud config configurations list, but this does not start the compute engine instances.\nRef: https://cloud.google.com/sdk/gcloud/reference/config/configurations/list\n\nExecute gcloud config configurations activate [config name] to activate the configuration for each project and execute gcloud configurations list to create and start the VM. is not right.\ngcloud configurations list is an invalid command. To list the existing named configurations, you need to execute gcloud config configurations list, but this does not start the compute engine instances.\nRef: https://cloud.google.com/sdk/gcloud/reference/config/configurations/list\n\nExecute gcloud config configurations activate [config name] to activate the configuration for each project and execute gcloud config list to create and start the VM. is not right.\ngcloud configurations activate [NAME] activates an existing named configuration. It can't be used to activate two configurations at the same time. Moreover, gcloud config list lists Cloud SDK properties for the currently active configuration. It does not start the Compute Engine instances.\nRef: https://cloud.google.com/sdk/gcloud/reference/config/configurations/activate\nRef: https://cloud.google.com/sdk/gcloud/reference/config/list\n\nExecute gcloud config configuration create [config name] to create two configurations, one for each project. Execute gcloud config configurations activate [config name] to activate the first configuration, and gcloud compute instances create to create the VM. Repeat the steps for other configuration. is the right answer.\n\nEach gcloud configuration has a 1 to 1 relationship with the region (if a region is defined). Since we have two different regions, we would need to create two separate configurations using gcloud config configurations create\nRef: https://cloud.google.com/sdk/gcloud/reference/config/configurations/create\n\nSecondly, you can activate each configuration independently by running gcloud config configurations activate [NAME]\nRef: https://cloud.google.com/sdk/gcloud/reference/config/configurations/activate\n\nFinally, while each configuration is active, you can run the gcloud compute instances start [NAME] command to start the instance in the configuration's region.\nRef: https://cloud.google.com/sdk/gcloud/reference/compute/instances/start"
    },
    {
      "options": [
        "Set the scope of the service account to Read/Write when provisioning compute engine instances in the ptech-vm project.",
        "Use gcloud to generate a JSON key for the existing service account used by the Cloud Function. Register the JSON key as SSH key on all VM instances in the ptech-vm project.",
        "Grant Compute Storage Admin IAM role on the ptech-vm project to the service account used by the Cloud Function.",
        "Use gcloud to generate a JSON key for the existing service account used by the Cloud Function. Add a metadata tag to all compute engine instances in the ptech-vm project with key: service-account and value: <JSON file contents>."
      ],
      "correct": "Grant Compute Storage Admin IAM role on the ptech-vm project to the service account used by the Cloud Function.",
      "questionText": "You have a Cloud Function that is triggered every night by Cloud Scheduler. The Cloud Function creates a snapshot of VMs running in all projects in the department. Your team created a new project ptech-vm, and you now need to provide IAM access to the service account used by the Cloud Function to let it create snapshots of VMs in the new ptech-vm project. You want to follow Google recommended practices. What should you do?",
      "explanation": "Use gcloud to generate a JSON key for the existing service account used by the Cloud Function. Add a metadata tag to all compute engine instances in the ptech-vm project with key: service-account and value: {JSON file contents}. is not right.\nAdding service accounts private key (JSON file) to VMs custom metadata does not affect the permissions granted to the Cloud Function’s service account. Metadata entries are key-value pairs and do not influence any other behaviour.\nRef: https://cloud.google.com/compute/docs/storing-retrieving-metadata\n\nUse gcloud to generate a JSON key for the existing service account used by the Cloud Function. Register the JSON key as SSH key on all VM instances in the ptech-vm project. is not right.\nAdding service accounts private key to the VMs SSH keys does not influence any other behaviour. SSH keys are used for SSHing to the instance.\nRef: https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys\n\nSet the scope of the service account to Read/Write when provisioning compute engine instances in the ptech-vm project. is not right.\nThe scopes can be modified when using compute engine default service account only.\nRef: https://cloud.google.com/compute/docs/access/service-accounts#default_service_account\n\nSee the screenshot below.\n\nThe scopes can not be modified when using a non-default service account. See the screenshot below.\n\nSince we want to use service accounts from another project, it is safe to say they are not the default compute service accounts of this project, and hence it is not possible to customize the scopes.\n\nGrant Compute Storage Admin IAM role on the ptech-vm project to the service account used by the Cloud Function. is the right answer.\nCompute Storage Admin role provides permissions to create, modify, and delete disks, images, and snapshots. If the service account in ptech-sa is granted the IAM Role of Compute Storage Admin in the project called ptech-vm, it can take snapshots and carry out other activities as defined by the role.\nRef: https://cloud.google.com/compute/docs/access/iam#compute.storageAdmin"
    },
    {
      "options": [
        "Store the data in a 2-node Cloud Spanner instance.",
        "Store the data in a multi-regional Cloud Spanner instance.",
        "Store the data in Cloud SQL for MySQL instance. Ensure Binary Logging on the Cloud SQL instance.",
        "Store the data in Highly Available Cloud SQL for MySQL instance."
      ],
      "correct": "Store the data in Cloud SQL for MySQL instance. Ensure Binary Logging on the Cloud SQL instance.",
      "questionText": "You work for a multinational consumer credit reporting company that collects and aggregates financial information and provides a credit report for over 100 million individuals and businesses. The company wants to trial a new application for a small geography and requires a relational database for storing important user information. Your company places a high value on reliability and requires point-in-time recovery while minimizing operational cost. What should you do?",
      "explanation": "\nStore the data in a 2-node Cloud Spanner instance. is not right.\nCloud spanner is a massively scalable, fully managed, relational database service for regional and global application data. Cloud spanner is expensive compared to Cloud SQL. We have a small set of data, and we want to be cost-effective, so Cloud Spanner doesn't fit these requirements. Furthermore, Cloud Spanner does not offer a \"Point in time\" recovery feature.\nRef: https://cloud.google.com/spanner\n\nStore the data in a multi-regional Cloud Spanner instance. is not right.\nCloud spanner is a massively scalable, fully managed, relational database service for regional and global application data. Cloud spanner is expensive compared to Cloud SQL. We don't require more than \"one geographic location\", and we want to be cost-effective, so Cloud Spanner doesn't fit these requirements. Furthermore, Cloud Spanner does not offer a \"Point in time\" recovery feature.\nRef: https://cloud.google.com/spanner\n\nStore the data in Highly Available Cloud SQL for MySQL instance. is not right.\nCloud SQL can easily handle small sets of relational data and is cost-effective compared to Cloud Spanner. But This option does not enable point in time recovery, so our requirement to support point-in-time recovery is not met.\nRef: https://cloud.google.com/sql/docs/mysql\n\nStore the data in Cloud SQL for MySQL instance. Ensure Binary Logging on the Cloud SQL instance. is the right answer.\nCloud SQL can easily handle small sets of relational data and is cost-effective compared to Cloud Spanner. And by enabling binary logging, we can enable point-in-time recovery, which fits our requirement.\n\nYou must enable binary logging to use point-in-time recovery. Point-in-time recovery helps you recover an instance to a specific point in time. For example, if an error causes a loss of data, you can recover a database to its state before the error occurred.\nRef: https://cloud.google.com/sql/docs/mysql/backup-recovery/backups#tips-pitr"
    },
    {
      "options": [
        "Update your GKE cluster to turn on GKE’s node auto-upgrade feature.",
        "When provisioning the GKE cluster, use Container Optimized OS node images.",
        "When provisioning the GKE cluster, ensure you use the latest stable and supported version.",
        "Update your GKE cluster to turn on GKE’s node auto-repair feature."
      ],
      "correct": "Update your GKE cluster to turn on GKE’s node auto-upgrade feature.",
      "questionText": "You are in the process of migrating a mission-critical application from your on-premises data centre to Google Kubernetes Engine (GKE). Your operations team do not want to take on the overhead for upgrading the GKE cluster and have asked you to ensure the Kubernetes version is always stable and supported. What should you do?",
      "explanation": "Update your GKE cluster to turn on GKE’s node auto-repair feature. is not right.\nGKE's node auto-repair feature helps you keep the nodes in your cluster in a healthy, running state. When enabled, GKE makes periodic checks on the health state of each node in your cluster. If a node fails consecutive health checks over an extended period, GKE initiates a repair process for that node.\nRef: https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-repair\n\nWhen provisioning the GKE cluster, ensure you use the latest stable and supported version. is not right.\nWe can select the latest available cluster version at the time of GKE cluster provisioning; however, this does not automatically upgrade the cluster if new versions become available.\n\nWhen provisioning the GKE cluster, use Container Optimized OS node images. is not right.\nContainer-Optimized OS comes with the Docker container runtime and all Kubernetes components pre-installed for out of the box deployment, management, and orchestration of your containers. But these do not help with automatically upgrading GKE cluster versions.\nRef: https://cloud.google.com/container-optimized-os\n\nUpdate your GKE cluster to turn on GKE’s node auto-upgrade feature. is the right answer.\nNode auto-upgrades help you keep the nodes in your cluster up to date with the cluster master version when your master is updated on your behalf. When you create a new cluster or node pool with Google Cloud Console or the gcloud command, node auto-upgrade is enabled by default.\nRef: https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-upgrades"
    },
    {
      "options": [
        "Export billing data to Google Cloud Storage bucket. Trigger a Cloud Function that reads the data and inserts into Cloud BigTable. Ask the budgeting team to run queries against BigTable to analyze current costs and estimate future costs.",
        "Export billing data to a BigQuery dataset. Ask the budgeting team to run queries against BigQuery to analyze current costs and estimate future costs.",
        "Export billing data to Google Cloud Storage bucket. Manually copy the data from Cloud Storage bucket to a Google sheet. Ask the budgeting team to apply formulas in the Google sheet to analyze current costs and estimate future costs.",
        "Download the costs as CSV file from the Cost Table page. Ask the budgeting team to open this file Microsoft Excel and apply formulas to analyze current costs and estimate future costs."
      ],
      "correct": "Export billing data to a BigQuery dataset. Ask the budgeting team to run queries against BigQuery to analyze current costs and estimate future costs.",
      "questionText": "Your company has three GCP projects – for development, test and production environments. The budgeting team in the finance department needs to know the cost estimates for the next financial year to include it in the budget. They have years of experience using SQL and need to group costs by parameters such as duration (day/week/month/quarter), service type, region, etc. How can you enable this?",
      "explanation": "Requirements\n\nuse query syntax\n\nneed the billing data of all three projects\n\nExport billing data to a Google Cloud Storage bucket. Trigger a Cloud Function that reads the data and inserts into Cloud BigTable. Ask the budgeting team to run queries against BigTable to analyze current costs and estimate future costs. is not right.\nBigTable is a NoSQL database and doesn't offer query syntax support.\n\nExport billing data to a Google Cloud Storage bucket. Manually copy the data from Cloud Storage bucket to a Google sheet. Ask the budgeting team to apply formulas in the Google sheet to analyze current costs and estimate future costs. is not right.\nGoogle Sheets don't offer full support for query syntax. Moreover, export to Cloud Storage bucket captures a smaller dataset than export to BigQuery. For example, the exported billing data does not include resource labels or any invoice-level charges such as taxes accrued or adjustment memos.\n\nDownload the costs as CSV file from the Cost Table page. Ask the budgeting team to open this file Microsoft Excel and apply formulas to analyze current costs and estimate future costs. is not right.\nBilling data can't be exported to a local file; it can only be exported to a BigQuery Dataset or Cloud Storage bucket.\n\nExport billing data to a BigQuery dataset. Ask the budgeting team to run queries against BigQuery to analyze current costs and estimate future costs. is the right answer.\nYou can export billing information from multiple projects into a BigQuery dataset. Unlike the export to Cloud Storage bucket, export to BigQuery dataset includes all information making it easy and straightforward to construct queries in BigQuery to estimate the cost. BigQuery supports Standard SQL so you can join tables and group by fields (labels in this case) as needed.\nRef: https://cloud.google.com/billing/docs/how-to/export-data-bigquery."
    },
    {
      "options": [
        "Enable a Cloud Trace on the bucket and wait for the user to access objects/set metadata to capture their activities.",
        "Retrieve this information from the Cloud Storage bucket page in GCP Console.",
        "Retrieve this information from Activity logs in GCP Console.",
        "Apply the necessary filters in Cloud Logging Console to retrieve this information."
      ],
      "correct": "Apply the necessary filters in Cloud Logging Console to retrieve this information.",
      "questionText": "Your company stores sensitive user information (PII) in three multi-regional buckets in US, Europe and Asia. All three buckets have data access logging enabled on them. The compliance team has received reports of fraudulent activity and has begun investigating a customer care representative. It believes the specific individual may have accessed some objects they are not authorized to and may have added labels to some files in the buckets to enable favourable discounts for their friends. The compliance team has asked you to provide them with a report of activities for this customer service representative on all three buckets. How can you do this efficiently?",
      "explanation": "Our requirements are - sensitive data, verify access, fewest possible steps.\n\nRetrieve this information from Activity logs in GCP Console. is not right.\nSince data access logging is enabled, you can see relevant log entries in both activity Logs as well as stack driver logs. However, verifying what has been viewed/updated is not straightforward in activity logs. Activity logs display a list of all actions, and you can restrict this down to a user and further filter by specifying Data access as the Activity types and GCS Bucket as the Resource type. But that is the extent of the filter functionality in Activity logs. It is not possible to restrict the activity logs to just the three interested buckets. Secondly, it is not possible to restrict the activity logs to just the gets and updates. So we'd have to go through the full list to identify activities of interest before verifying them which is a manual process and can be time taking depending on the number of activities in the list.\nRef: https://cloud.google.com/storage/docs/audit-logs\n\nRetrieve this information from the Cloud Storage bucket page in GCP Console. is not right.\nThe bucket page in the GCP console does not show the logs.\n\n\n\n\nEnable a Cloud Trace on the bucket and wait for the user to access objects/set metadata to capture their activities. is not right.\nCloud Trace is not supported on Google Cloud Storage. Stackdriver Trace runs on Linux in the following environments: Compute Engine, Google Kubernetes Engine (GKE), App Engine flexible environment, App Engine standard environment.\nRef: https://cloud.google.com/trace/docs/overview\n\nApply the necessary filters in Cloud Logging Console to retrieve this information. is the right answer.\nData access logs are already enabled, so we already record all API calls that read the configuration or metadata of resources, as well as user-driven API calls that create, modify, or read user-provided resource data. Data Access audit logs do not record the data-access operations on resources that are publicly shared (available to All Users or All Authenticated Users), or that can be accessed without logging into Google Cloud.\n\nSince we are dealing with sensitive data, it is safe to assume that these buckets are not publicly shared and therefore enabling Data access logging logs all data-access operations on resources. These logs are sent to Stackdriver where they can be viewed by applying a suitable filter.\n\nUnlike activity logs, retrieving the required information to verify is quicker through Stackdriver as you can apply filters such as\n\nresource.type=\"gcs_bucket\"\n(resource.labels.bucket_name=\"gcp-ace-lab-255520\" OR resource.labels.bucket_name=\"gcp-ace-lab-255521\" OR resource.labels.bucket_name=\"gcp-ace-lab-255522\")\n(protoPayload.methodName=\"storage.objects.get\" OR protoPayload.methodName=\"storage.objects.update\")\nprotoPayload.authenticationInfo.principalEmail=\"test.gcp.labs.user@gmail.com\"\n\nand query just the gets and updates, for specific buckets for a specific user. This option involves fewer steps and is more efficient. Data access logging is not enabled by default and needs to be enabled explicitly. The screenshot below shows a screenshot for enabling the data access logging for Google Cloud Storage.\n\n\n"
    },
    {
      "options": [
        "In the App Engine Console, identify the App Engine application and select Revert.",
        "In the App Engine Console, identify the App Engine application versions and make the previous version the default to route all traffic to it.",
        "Execute gcloud app restore to rollback to the previous version.",
        "Deploy the previous version as a new App Engine Application and use traffic splitting feature to send all traffic to the new application."
      ],
      "correct": "In the App Engine Console, identify the App Engine application versions and make the previous version the default to route all traffic to it.",
      "questionText": "You developed an enhancement to a production application deployed in App Engine Standard service. Unit testing and user acceptance testing has succeeded, and you deployed the new version to production. Users have started complaining of slow performance after the recent update, and you need to revert to the previous version immediately. How can you do this?",
      "explanation": "Execute gcloud app restore to rollback to the previous version. is not right.\nrestore action is not supported by gcloud app command.\nRef: https://cloud.google.com/sdk/gcloud/reference/app/deploy\n\nIn the App Engine Console, identify the App Engine application and select Revert. is not right.\nRevert option is not present on the App Engine page of the GCP Console.\n\nDeploy the previous version as a new App Engine Application and use traffic splitting feature to send all traffic to the new application. is not right.\nEach application in the app engine is different, and it is not possible to split traffic between applications in App Engine. You can use traffic splitting to specify a percentage distribution of traffic across two or more of the versions within a service but not across applications.\nRef: https://cloud.google.com/appengine/docs/standard/python/splitting-traffic\n\nIn the App Engine Console, identify the App Engine application versions and make the previous version the default to route all traffic to it. is the right answer\nYou can roll back to a previous version in the app engine GCP console. Go back to the list of versions and check the box next to the version that you want to receive all traffic and click the MAKE DEFAULT button located above the list. Traffic immediately switches over to the selected version.\nRef: https://cloud.google.com/community/tutorials/how-to-roll-your-app-engine-managed-vms-app-back-to-a-previous-version-part-1"
    },
    {
      "options": [
        "Grant roles/spanner.databaseUser IAM role to all operations engineers group.",
        "Grant roles/spanner.databaseUser IAM role to all operations engineers user accounts.",
        "Grant roles/spanner.viewer IAM role to all operations engineers user accounts.",
        "Grant roles/spanner.viewer IAM role to all operations engineers group."
      ],
      "correct": "Grant roles/spanner.databaseUser IAM role to all operations engineers group.",
      "questionText": "Your company’s new mobile game has gone live, and you have transitioned the backend application to the operations team. The mobile game uses Cloud Spanner to persist game state, leaderboard and player profile. All operations engineers require access to view and edit table data to support runtime issues. What should you do?",
      "explanation": "Our requirements\n\nView and Edit table data\n\nMultiple users\n\nMultiple users indicate that we do not want to assign roles/permissions at the user level. Instead, we should do it based on groups so that we can create one group with all the required permissions and all such users who need this access can be assigned to the group.\nRef: https://cloud.google.com/iam/docs/reference/rest/v1/Policy#Binding\nRef: https://cloud.google.com/iam/docs/granting-changing-revoking-access#granting-gcloud-manual\n\nGrant roles/spanner.databaseUser IAM role to all operations engineers user accounts. is not right.\nWe are looking for an option that assigns users to a group (to maximise reuse and minimize maintenance overhead). This option assigns users to the role so is not the right answer.\n\nGrant roles/spanner.viewer IAM role to all operations engineers user accounts. is not right.\nWe are looking for an option that assigns users to a group (to maximise reuse and minimize maintenance overhead). This option assigns users to the role so is not the right answer.\n\nGrant roles/spanner.viewer IAM role to all operations engineers group. is not right.\nAdding users to a group and granting the role to the group is the right way forward. But the role used in this option is spanner.viewer which allows viewing all Cloud Spanner instances (but cannot modify instances). It allows viewing all Cloud Spanner databases (but cannot modify databases and cannot read from databases). Since we required edit access as well, this option is not right.\nRef: https://cloud.google.com/spanner/docs/iam\n\nGrant roles/spanner.databaseUser IAM role to all operations engineers group. is the right answer.\nAdding users to a group and granting the role to the group is the right way forward. Also, we assign the role spanner.databaseUser which allows Read from and write to the Cloud Spanner database; execute SQL queries on the database, including DML and Partitioned DML; and View and update schema for the database. This option grants the right role to a group and assigns users to the group."
    }
  ]